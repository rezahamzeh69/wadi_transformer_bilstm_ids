{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3R_qUExqcoXl",
        "outputId": "640e4eb8-b33e-491c-be4d-8dbd3f1da3ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved: ./wadi_preprocessed/wadi_unswized_train.csv ./wadi_preprocessed/wadi_unswized_val.csv ./wadi_preprocessed/wadi_unswized_test.csv\n",
            "Shapes: (264, 838) (38, 838) (76, 838)\n",
            "Class ratios (train/val/test): 0.5 0.5 0.5\n"
          ]
        }
      ],
      "source": [
        "import os, math, gc\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "OUT_DIR = \"./wadi_preprocessed\"\n",
        "SRC = f\"{OUT_DIR}/wadi_test_preprocessed.csv\"\n",
        "assert Path(SRC).exists(), \"ابتدا پیش‌پردازش WADI را اجرا کنید.\"\n",
        "\n",
        "SEQ_LEN   = 60\n",
        "STRIDE    = 5\n",
        "DILATE    = 5\n",
        "NEG_RATIO = 1.0\n",
        "NON_OVERLAP = True\n",
        "HARD_NEG_Q = 0.75\n",
        "\n",
        "df = pd.read_csv(SRC)\n",
        "FEATURES = [c for c in df.columns if c != \"label\"]\n",
        "X = df[FEATURES].astype(np.float32).values\n",
        "y = df[\"label\"].astype(np.int64).values\n",
        "del df; gc.collect()\n",
        "\n",
        "if DILATE > 0:\n",
        "    k = np.ones(2*DILATE+1, dtype=np.int32)\n",
        "    y = (np.convolve(y, k, mode='same') > 0).astype(np.int64)\n",
        "\n",
        "n_seq = len(X) - SEQ_LEN + 1\n",
        "starts = np.arange(0, n_seq, STRIDE, dtype=np.int64)\n",
        "\n",
        "from numpy.lib.stride_tricks import sliding_window_view\n",
        "y_win_full = (sliding_window_view(y, SEQ_LEN).max(axis=1)).astype(np.int64)\n",
        "y_win_sel = y_win_full[starts]\n",
        "\n",
        "pos_idx = starts[y_win_sel==1].copy()\n",
        "neg_idx = starts[y_win_sel==0].copy()\n",
        "\n",
        "diff_act = None\n",
        "try:\n",
        "    diffs = np.abs(np.diff(X, axis=0))\n",
        "    act = diffs.mean(axis=1)\n",
        "    act_win = sliding_window_view(act, SEQ_LEN-1).mean(axis=1)\n",
        "    diff_act = act_win[starts]\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "n_pos = len(pos_idx)\n",
        "if n_pos == 0:\n",
        "    raise RuntimeError(\"پنجره‌ی مثبت ندارید؛ SEQ_LEN را کم یا DILATE را زیاد کنید.\")\n",
        "n_neg = int(max(1, math.ceil(NEG_RATIO * n_pos)))\n",
        "\n",
        "if diff_act is not None:\n",
        "    neg_scores = diff_act[y_win_sel==0]\n",
        "    thr = np.quantile(neg_scores, HARD_NEG_Q)\n",
        "    hard_neg = neg_idx[neg_scores >= thr]\n",
        "    if len(hard_neg) >= n_neg:\n",
        "        sel_neg = np.random.choice(hard_neg, size=n_neg, replace=False)\n",
        "    else:\n",
        "        rest = np.setdiff1d(neg_idx, hard_neg)\n",
        "        add = np.random.choice(rest, size=max(0, n_neg-len(hard_neg)), replace=False)\n",
        "        sel_neg = np.concatenate([hard_neg, add])\n",
        "else:\n",
        "    sel_neg = np.random.choice(neg_idx, size=n_neg, replace=False)\n",
        "\n",
        "def make_non_overlapping(idxs):\n",
        "    if not NON_OVERLAP: return idxs\n",
        "    idxs = np.sort(idxs)\n",
        "    picked, last_end = [], -1\n",
        "    for s in idxs:\n",
        "        if s >= last_end:\n",
        "            picked.append(s)\n",
        "            last_end = s + SEQ_LEN\n",
        "    return np.array(picked, dtype=np.int64)\n",
        "\n",
        "pos_idx = make_non_overlapping(pos_idx)\n",
        "sel_neg = make_non_overlapping(sel_neg)\n",
        "n = min(len(pos_idx), len(sel_neg))\n",
        "pos_idx = pos_idx[:n]; sel_neg = sel_neg[:n]\n",
        "\n",
        "starts_final = np.concatenate([pos_idx, sel_neg])\n",
        "labels_final = np.concatenate([np.ones(len(pos_idx), dtype=np.int64),\n",
        "                               np.zeros(len(sel_neg), dtype=np.int64)])\n",
        "order = np.random.permutation(len(starts_final))\n",
        "starts_final = starts_final[order]; labels_final = labels_final[order]\n",
        "\n",
        "def aggregate_window(feat_block):\n",
        "    mean  = feat_block.mean(axis=0)\n",
        "    std   = feat_block.std(axis=0, ddof=0)\n",
        "    minv  = feat_block.min(axis=0)\n",
        "    maxv  = feat_block.max(axis=0)\n",
        "    last  = feat_block[-1]\n",
        "    first = feat_block[0]\n",
        "    slope = np.polyfit(np.arange(len(feat_block)), feat_block, deg=1)[0]\n",
        "    iqr   = np.quantile(feat_block, 0.75, axis=0) - np.quantile(feat_block, 0.25, axis=0)\n",
        "    dsum  = np.abs(np.diff(feat_block, axis=0)).sum(axis=0)\n",
        "    return np.concatenate([mean, std, minv, maxv, last, last-first, slope, iqr, dsum], axis=0)\n",
        "\n",
        "blocks = [aggregate_window(X[s:s+SEQ_LEN, :]) for s in starts_final]\n",
        "X_agg = np.stack(blocks, axis=0)\n",
        "\n",
        "cols = []\n",
        "for prefix in [\"mean\",\"std\",\"min\",\"max\",\"last\",\"delta\",\"slope\",\"iqr\",\"dsum\"]:\n",
        "    cols += [f\"{prefix}_{f}\" for f in FEATURES]\n",
        "df_out = pd.DataFrame(X_agg, columns=cols)\n",
        "df_out[\"label\"] = labels_final\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "train_df, temp_df = train_test_split(df_out, test_size=0.3, random_state=42, stratify=df_out[\"label\"])\n",
        "val_df, test_df   = train_test_split(temp_df, test_size=2/3, random_state=42, stratify=temp_df[\"label\"])\n",
        "\n",
        "Path(OUT_DIR).mkdir(parents=True, exist_ok=True)\n",
        "train_path = f\"{OUT_DIR}/wadi_unswized_train.csv\"\n",
        "val_path   = f\"{OUT_DIR}/wadi_unswized_val.csv\"\n",
        "test_path  = f\"{OUT_DIR}/wadi_unswized_test.csv\"\n",
        "train_df.to_csv(train_path, index=False)\n",
        "val_df.to_csv(val_path, index=False)\n",
        "test_df.to_csv(test_path, index=False)\n",
        "print(\"Saved:\", train_path, val_path, test_path)\n",
        "print(\"Shapes:\", train_df.shape, val_df.shape, test_df.shape)\n",
        "print(\"Class ratios (train/val/test):\", train_df['label'].mean(), val_df['label'].mean(), test_df['label'].mean())\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, re, sys, math, time, gc, subprocess, warnings\n",
        "from pathlib import Path\n",
        "from typing import Optional, List, Tuple\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
        "\n",
        "SEED=42\n",
        "np.random.seed(SEED); torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available(): torch.cuda.manual_seed_all(SEED)\n",
        "DEVICE=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Device:\", DEVICE)\n",
        "\n",
        "# ====================== Config ======================\n",
        "KAGGLE_SLUG = \"giovannimonco/wadi-data\"\n",
        "RAW_OUT_DIR = \"./wadi_preprocessed\"\n",
        "Path(RAW_OUT_DIR).mkdir(parents=True, exist_ok=True)\n",
        "TRAIN_CSV = f\"{RAW_OUT_DIR}/wadi_train_preprocessed.csv\"\n",
        "TEST_CSV  = f\"{RAW_OUT_DIR}/wadi_test_preprocessed.csv\"\n",
        "\n",
        "# UNSW-ization\n",
        "SEQ_LEN=60\n",
        "STRIDE=5\n",
        "DILATE=5\n",
        "NEG_RATIO=1.0\n",
        "NON_OVERLAP=True\n",
        "HARD_NEG_Q=0.75\n",
        "\n",
        "# Model+train\n",
        "MODEL_TYPE=\"trf_bilstm_sensors\"   # \"mlp\" یا \"trf_bilstm_sensors\"\n",
        "BATCH_SIZE=256 if DEVICE==\"cpu\" else 512\n",
        "EPOCHS=15\n",
        "LR=3e-4\n",
        "DROPOUT=0.1\n",
        "\n",
        "# Transformer across sensors\n",
        "TR_DIM=128\n",
        "NHEAD=4\n",
        "N_LAY_TR=2\n",
        "LSTM_H=128\n",
        "N_LAY_LSTM=1\n",
        "\n",
        "CKPT_DIR=\"./bilstm_trf_checkpoints\"; os.makedirs(CKPT_DIR, exist_ok=True)\n",
        "CKPT=f\"{CKPT_DIR}/best_unswized_{MODEL_TYPE}.pt\"\n",
        "\n",
        "# ====================== Preprocess (WADI.ipynb-like) ======================\n",
        "def strip_and_dedup(df):\n",
        "    df.columns=[re.sub(r\"\\s+\",\" \",str(c).strip()) for c in df.columns]\n",
        "    return df.loc[:,~df.columns.duplicated()]\n",
        "\n",
        "def read_train_csv(p):\n",
        "    name=Path(p).name.lower()\n",
        "    df=pd.read_csv(p, skiprows=4, low_memory=False) if (\"14days.csv\" in name and \"new\" not in name) else pd.read_csv(p, low_memory=False)\n",
        "    return strip_and_dedup(df)\n",
        "\n",
        "def read_test_csv(p):\n",
        "    name=Path(p).name.lower()\n",
        "    try:\n",
        "        df=pd.read_csv(p, skiprows=1, low_memory=False) if \"lable\" in name else pd.read_csv(p, low_memory=False)\n",
        "    except Exception:\n",
        "        df=pd.read_csv(p, low_memory=False)\n",
        "    df=strip_and_dedup(df)\n",
        "    if df.iloc[0].astype(str).str.contains(\"Row|Date|Time|Attack\",case=False,regex=True).any():\n",
        "        df.columns=df.iloc[0].astype(str).tolist()\n",
        "        df=strip_and_dedup(df.iloc[1:].reset_index(drop=True))\n",
        "    return df\n",
        "\n",
        "def extract_label_from_test(df):\n",
        "    cands=[c for c in df.columns if re.search(r\"(label|lable|attack)\",c,flags=re.I)]\n",
        "    lab=None\n",
        "    for c in cands:\n",
        "        col=pd.to_numeric(df[c], errors=\"coerce\")\n",
        "        u=set(col.dropna().unique().tolist())\n",
        "        if u.issubset({-1,0,1}) and len(u)>0:\n",
        "            lab=col.map({1:0,-1:1,0:0}); break\n",
        "    if lab is None: lab=pd.Series(np.zeros(len(df),dtype=np.int64), name=\"label\")\n",
        "    return lab.astype(\"int64\").rename(\"label\")\n",
        "\n",
        "def numeric_common(train_df, test_df):\n",
        "    drop={\"Date\",\"Time\",\"datetime\",\"timestamp\",\"Row\",\"row\",\"index\"}\n",
        "    num_tr=[c for c in train_df.columns if pd.api.types.is_numeric_dtype(train_df[c]) and c not in drop]\n",
        "    num_te=[c for c in test_df.columns  if pd.api.types.is_numeric_dtype(test_df[c]) and c not in drop]\n",
        "    feats=sorted(list(set(num_tr)&set(num_te)))\n",
        "    if not feats: raise RuntimeError(\"No shared numeric columns.\")\n",
        "    Xtr=train_df[feats].copy(); Xte=test_df[feats].copy()\n",
        "    for X in (Xtr,Xte):\n",
        "        X.replace([\"?\",\"NA\",\"NaN\",\"nan\",\"\"], np.nan, inplace=True)\n",
        "        for c in X.columns: X[c]=pd.to_numeric(X[c], errors=\"coerce\")\n",
        "        X.interpolate(limit_direction=\"both\", inplace=True)\n",
        "        X.ffill(inplace=True); X.bfill(inplace=True)\n",
        "    return Xtr, Xte, feats\n",
        "\n",
        "def safe_scale(Xtr, Xte, eps=1e-8):\n",
        "    std=Xtr.std(0, ddof=0); keep=std[std>eps].index.tolist()\n",
        "    if len(keep)<len(std): print(f\"[info] dropped {len(std)-len(keep)} low-variance cols\")\n",
        "    mu=Xtr[keep].mean(0); sd=Xtr[keep].std(0, ddof=0).clip(lower=eps)\n",
        "    Xtr=((Xtr[keep]-mu)/sd).astype(\"float32\").replace([np.inf,-np.inf],np.nan).fillna(0.0)\n",
        "    Xte=((Xte[keep]-mu)/sd).astype(\"float32\").replace([np.inf,-np.inf],np.nan).fillna(0.0)\n",
        "    return Xtr, Xte, keep\n",
        "\n",
        "def find_file(cands, root):\n",
        "    low=[c.lower() for c in cands]\n",
        "    for c in cands:\n",
        "        p=Path(root)/c\n",
        "        if p.exists(): return str(p)\n",
        "    for r,_,files in os.walk(root):\n",
        "        for f in files:\n",
        "            if f.lower() in low: return str(Path(r)/f)\n",
        "    for r,_,files in os.walk(root):\n",
        "        for f in files:\n",
        "            fl=f.lower()\n",
        "            for cand in low:\n",
        "                if cand.replace(\"_\",\"\").replace(\"-\",\"\") in fl.replace(\"_\",\"\").replace(\"-\",\"\"):\n",
        "                    return str(Path(r)/f)\n",
        "    return None\n",
        "\n",
        "def download_or_cache():\n",
        "    try:\n",
        "        import kagglehub\n",
        "    except Exception:\n",
        "        subprocess.check_call([sys.executable,\"-m\",\"pip\",\"install\",\"-q\",\"kagglehub\"]); import kagglehub\n",
        "    print(\"➜ kagglehub...\")\n",
        "    p=kagglehub.dataset_download(KAGGLE_SLUG)\n",
        "    print(\"[ok]\", p); return p\n",
        "\n",
        "def ensure_preprocessed():\n",
        "    if os.path.exists(TEST_CSV) and os.path.exists(TRAIN_CSV):\n",
        "        print(\"✓ CSVهای از قبل موجودند.\"); return\n",
        "    print(\"➜ دانلود WADI ...\")\n",
        "    root=download_or_cache()\n",
        "    tr=find_file([\"WADI_14days_new.csv\",\"WADI_14days.csv\"], root)\n",
        "    te=find_file([\"WADI_attackdataLABLE.csv\",\"WADI_attackdata.csv\"], root)\n",
        "    if not tr or not te: raise FileNotFoundError(\"WADI CSVs not found\")\n",
        "    print(\"✓\", tr); print(\"✓\", te)\n",
        "    tr_df=read_train_csv(tr); te_df=read_test_csv(te)\n",
        "    ytest=extract_label_from_test(te_df)\n",
        "    Xtr_raw,Xte_raw,feats=numeric_common(tr_df, te_df)\n",
        "    Xtr,Xte,feats_kept=safe_scale(Xtr_raw, Xte_raw)\n",
        "    tr_out=Xtr.copy(); tr_out[\"label\"]=0\n",
        "    te_out=Xte.copy(); te_out[\"label\"]=ytest.values\n",
        "    tr_out.to_csv(TRAIN_CSV,index=False); te_out.to_csv(TEST_CSV,index=False)\n",
        "    print(\"✓ ساخته شد:\", TRAIN_CSV, TEST_CSV, \"| n_feats:\", len(feats_kept))\n",
        "\n",
        "ensure_preprocessed()\n",
        "\n",
        "# ====================== UNSW-ize WADI ======================\n",
        "SRC=TEST_CSV\n",
        "df=pd.read_csv(SRC)\n",
        "FEATURES=[c for c in df.columns if c!=\"label\"]\n",
        "X=df[FEATURES].astype(np.float32).values\n",
        "y=df[\"label\"].astype(np.int64).values\n",
        "del df; gc.collect()\n",
        "\n",
        "if DILATE>0:\n",
        "    k=np.ones(2*DILATE+1, dtype=np.int32)\n",
        "    y=(np.convolve(y, k, mode=\"same\")>0).astype(np.int64)\n",
        "\n",
        "n_seq=len(X)-SEQ_LEN+1\n",
        "from numpy.lib.stride_tricks import sliding_window_view\n",
        "starts=np.arange(0, n_seq, STRIDE, dtype=np.int64)\n",
        "y_win_full=(sliding_window_view(y, SEQ_LEN).max(axis=1)).astype(np.int64)\n",
        "y_sel=y_win_full[starts]\n",
        "pos_idx=starts[y_sel==1].copy()\n",
        "neg_idx=starts[y_sel==0].copy()\n",
        "\n",
        "diff_act=None\n",
        "try:\n",
        "    diffs=np.abs(np.diff(X,axis=0))\n",
        "    act=diffs.mean(axis=1)\n",
        "    act_win=sliding_window_view(act, SEQ_LEN-1).mean(axis=1)\n",
        "    diff_act=act_win[starts]\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "n_pos=len(pos_idx)\n",
        "if n_pos==0: raise RuntimeError(\"پنجره‌ی مثبت ندارید؛ SEQ_LEN را کم یا DILATE را زیاد کنید.\")\n",
        "n_neg=int(max(1, math.ceil(NEG_RATIO*n_pos)))\n",
        "\n",
        "if diff_act is not None:\n",
        "    neg_scores=diff_act[y_sel==0]\n",
        "    thr=np.quantile(neg_scores, HARD_NEG_Q)\n",
        "    hard_neg=neg_idx[neg_scores>=thr]\n",
        "    if len(hard_neg)>=n_neg:\n",
        "        sel_neg=np.random.choice(hard_neg, size=n_neg, replace=False)\n",
        "    else:\n",
        "        rest=np.setdiff1d(neg_idx, hard_neg)\n",
        "        add=np.random.choice(rest, size=max(0,n_neg-len(hard_neg)), replace=False)\n",
        "        sel_neg=np.concatenate([hard_neg, add])\n",
        "else:\n",
        "    sel_neg=np.random.choice(neg_idx, size=n_neg, replace=False)\n",
        "\n",
        "def make_non_overlapping(idxs):\n",
        "    if not NON_OVERLAP: return idxs\n",
        "    idxs=np.sort(idxs)\n",
        "    picked=[]; last_end=-1\n",
        "    for s in idxs:\n",
        "        if s>=last_end:\n",
        "            picked.append(s); last_end=s+SEQ_LEN\n",
        "    return np.array(picked, dtype=np.int64)\n",
        "\n",
        "pos_idx=make_non_overlapping(pos_idx)\n",
        "sel_neg=make_non_overlapping(sel_neg)\n",
        "m=min(len(pos_idx), len(sel_neg))\n",
        "pos_idx=pos_idx[:m]; sel_neg=sel_neg[:m]\n",
        "\n",
        "starts_final=np.concatenate([pos_idx, sel_neg])\n",
        "labels_final=np.concatenate([np.ones(len(pos_idx),dtype=np.int64), np.zeros(len(sel_neg),dtype=np.int64)])\n",
        "perm=np.random.permutation(len(starts_final))\n",
        "starts_final=starts_final[perm]; labels_final=labels_final[perm]\n",
        "\n",
        "def aggregate_window(block):\n",
        "    mean=block.mean(0); std=block.std(0,ddof=0); minv=block.min(0); maxv=block.max(0)\n",
        "    last=block[-1]; delta=last-block[0]\n",
        "    t=np.arange(len(block))\n",
        "    slope=np.polyfit(t, block, deg=1)[0]\n",
        "    q75=np.quantile(block,0.75,axis=0); q25=np.quantile(block,0.25,axis=0); iqr=q75-q25\n",
        "    dsum=np.abs(np.diff(block,axis=0)).sum(0)\n",
        "    return np.concatenate([mean,std,minv,maxv,last,delta,slope,iqr,dsum], axis=0)\n",
        "\n",
        "blocks=[aggregate_window(X[s:s+SEQ_LEN,:]) for s in starts_final]\n",
        "X_agg=np.stack(blocks, axis=0)\n",
        "\n",
        "stat_prefixes=[\"mean\",\"std\",\"min\",\"max\",\"last\",\"delta\",\"slope\",\"iqr\",\"dsum\"]\n",
        "cols=[]\n",
        "for pfx in stat_prefixes:\n",
        "    cols += [f\"{pfx}_{f}\" for f in FEATURES]\n",
        "df_out=pd.DataFrame(X_agg, columns=cols); df_out[\"label\"]=labels_final\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "train_df, tmp_df = train_test_split(df_out, test_size=0.3, random_state=SEED, stratify=df_out[\"label\"])\n",
        "val_df, test_df  = train_test_split(tmp_df, test_size=2/3, random_state=SEED, stratify=tmp_df[\"label\"])\n",
        "\n",
        "UNSW_TRAIN=f\"{RAW_OUT_DIR}/wadi_unswized_train.csv\"\n",
        "UNSW_VAL  =f\"{RAW_OUT_DIR}/wadi_unswized_val.csv\"\n",
        "UNSW_TEST =f\"{RAW_OUT_DIR}/wadi_unswized_test.csv\"\n",
        "train_df.to_csv(UNSW_TRAIN, index=False); val_df.to_csv(UNSW_VAL, index=False); test_df.to_csv(UNSW_TEST, index=False)\n",
        "print(\"UNSW-ized CSVs:\", UNSW_TRAIN, UNSW_VAL, UNSW_TEST)\n",
        "print(\"Shapes:\", train_df.shape, val_df.shape, test_df.shape)\n",
        "print(\"Label ratio train/val/test:\", train_df[\"label\"].mean(), val_df[\"label\"].mean(), test_df[\"label\"].mean())\n",
        "\n",
        "# ====================== Scaling ======================\n",
        "all_cols=[c for c in train_df.columns if c!=\"label\"]\n",
        "Xtr=train_df[all_cols].values.astype(np.float32)\n",
        "Xva=val_df[all_cols].values.astype(np.float32)\n",
        "Xte=test_df[all_cols].values.astype(np.float32)\n",
        "ytr=train_df[\"label\"].values.astype(np.int64)\n",
        "yva=val_df[\"label\"].values.astype(np.int64)\n",
        "yte=test_df[\"label\"].values.astype(np.int64)\n",
        "\n",
        "mu=Xtr.mean(0); sd=Xtr.std(0, ddof=0).clip(min=1e-8)\n",
        "Xtr=((Xtr-mu)/sd).astype(np.float32)\n",
        "Xva=((Xva-mu)/sd).astype(np.float32)\n",
        "Xte=((Xte-mu)/sd).astype(np.float32)\n",
        "\n",
        "# ====================== Datasets for two model types ======================\n",
        "sensor_names=FEATURES[:]  # original sensor list\n",
        "S=len(sensor_names); D=len(stat_prefixes)\n",
        "\n",
        "def build_sensor_token_tensor(flat_row: np.ndarray):\n",
        "    m={}\n",
        "    for j, col in enumerate(all_cols):\n",
        "        pfx, _, feat = col.partition(\"_\")\n",
        "        if feat not in m: m[feat]={}\n",
        "        m[feat][pfx]=j\n",
        "    tokens=[]\n",
        "    for feat in sensor_names:\n",
        "        if feat not in m: continue\n",
        "        idxs=[m[feat].get(p, None) for p in stat_prefixes]\n",
        "        if any(i is None for i in idxs):\n",
        "            continue\n",
        "        tokens.append(flat_row[idxs])\n",
        "    if len(tokens)==0:\n",
        "        return None\n",
        "    T=np.stack(tokens, axis=0)  # [S_eff, D]\n",
        "    return T\n",
        "\n",
        "class DSMLP(Dataset):\n",
        "    def __init__(self, X, y): self.X=torch.from_numpy(X); self.y=torch.from_numpy(y).float()\n",
        "    def __len__(self): return len(self.y)\n",
        "    def __getitem__(self,i): return self.X[i], self.y[i]\n",
        "\n",
        "class DSSensors(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.y=torch.from_numpy(y).float()\n",
        "        xs=[]\n",
        "        for i in range(len(X)):\n",
        "            T=build_sensor_token_tensor(X[i])\n",
        "            if T is None: continue\n",
        "            xs.append(torch.from_numpy(T.astype(np.float32)))\n",
        "        self.X=xs\n",
        "        if len(self.X)!=len(self.y):\n",
        "            self.y=self.y[:len(self.X)]\n",
        "    def __len__(self): return len(self.y)\n",
        "    def __getitem__(self,i): return self.X[i], self.y[i]\n",
        "\n",
        "if MODEL_TYPE==\"mlp\":\n",
        "    tr_ds=DSMLP(Xtr, ytr); va_ds=DSMLP(Xva, yva); te_ds=DSMLP(Xte, yte)\n",
        "else:\n",
        "    tr_ds=DSSensors(Xtr, ytr); va_ds=DSSensors(Xva, yva); te_ds=DSSensors(Xte, yte)\n",
        "\n",
        "train_loader=DataLoader(tr_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=0, collate_fn=None)\n",
        "val_loader  =DataLoader(va_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0, collate_fn=None)\n",
        "test_loader =DataLoader(te_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0, collate_fn=None)\n",
        "\n",
        "# ====================== Models ======================\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, in_dim, p=0.2):\n",
        "        super().__init__()\n",
        "        self.net=nn.Sequential(\n",
        "            nn.Linear(in_dim, 512), nn.ReLU(), nn.Dropout(p),\n",
        "            nn.Linear(512, 256), nn.ReLU(), nn.Dropout(p),\n",
        "            nn.Linear(256, 1)\n",
        "        )\n",
        "    def forward(self,x): return self.net(x).squeeze(1)\n",
        "\n",
        "class SinusoidalPositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=2048):\n",
        "        super().__init__()\n",
        "        pe=torch.zeros(max_len, d_model, dtype=torch.float32)\n",
        "        pos=torch.arange(0,max_len,dtype=torch.float32).unsqueeze(1)\n",
        "        div=torch.exp(torch.arange(0,d_model,2,dtype=torch.float32)*(-math.log(10000.0)/d_model))\n",
        "        pe[:,0::2]=torch.sin(pos*div); pe[:,1::2]=torch.cos(pos*div)\n",
        "        self.register_buffer(\"pe\", pe.unsqueeze(0))\n",
        "    def forward(self,x): return x + self.pe[:, :x.size(1), :]\n",
        "\n",
        "class TrEncLayer(nn.Module):\n",
        "    def __init__(self,d,nhead,ff,p):\n",
        "        super().__init__()\n",
        "        self.attn=nn.MultiheadAttention(d,nhead,dropout=p,batch_first=True)\n",
        "        self.l1, self.l2=nn.Linear(d,ff), nn.Linear(ff,d)\n",
        "        self.n1, self.n2=nn.LayerNorm(d), nn.LayerNorm(d)\n",
        "        self.d1, self.d2=nn.Dropout(p), nn.Dropout(p)\n",
        "        self.act=nn.GELU()\n",
        "    def forward(self,x):\n",
        "        a,_=self.attn(x,x,x,need_weights=False)\n",
        "        x=self.n1(x + self.d1(a))\n",
        "        x=self.n2(x + self.d2(self.l2(self.d1(self.act(self.l1(x))))))\n",
        "        return x\n",
        "\n",
        "class TrEnc(nn.Module):\n",
        "    def __init__(self,d,nhead,nlayer,ff,p):\n",
        "        super().__init__()\n",
        "        self.layers=nn.ModuleList([TrEncLayer(d,nhead,ff,p) for _ in range(nlayer)])\n",
        "    def forward(self,x):\n",
        "        for lyr in self.layers: x=lyr(x)\n",
        "        return x\n",
        "\n",
        "class TrBiLSTM_Sensors(nn.Module):\n",
        "    def __init__(self, token_dim, d_model, nhead, nlay_tr, lstm_h, nlay_lstm, p=0.1):\n",
        "        super().__init__()\n",
        "        self.proj=nn.Linear(token_dim, d_model)\n",
        "        self.pos =SinusoidalPositionalEncoding(d_model)\n",
        "        self.enc =TrEnc(d_model, nhead, nlay_tr, d_model*2, p)\n",
        "        self.lstm=nn.LSTM(d_model, lstm_h, num_layers=nlay_lstm, batch_first=True, bidirectional=True, dropout=p if nlay_lstm>1 else 0.0)\n",
        "        self.drop=nn.Dropout(p)\n",
        "        self.fc  =nn.Linear(lstm_h*2, 1)\n",
        "    def forward(self, x):\n",
        "        x=self.proj(x)            # [B,S,Dm]\n",
        "        x=self.pos(x)\n",
        "        x=self.enc(x)\n",
        "        x,_=self.lstm(x)\n",
        "        x=x.mean(1)               # mean over sensors\n",
        "        x=self.drop(x)\n",
        "        return self.fc(x).squeeze(1)\n",
        "\n",
        "# collate for variable S across items (rare if sensors fixed)\n",
        "def pad_collate(batch):\n",
        "    xs, ys = zip(*batch)\n",
        "    Smax=max(t.shape[0] for t in xs)\n",
        "    D=xs[0].shape[1]\n",
        "    padded=torch.zeros(len(xs), Smax, D, dtype=torch.float32)\n",
        "    for i,t in enumerate(xs):\n",
        "        padded[i,:t.shape[0],:]=t\n",
        "    return padded.to(DEVICE), torch.stack(ys).to(DEVICE)\n",
        "\n",
        "# pick model\n",
        "if MODEL_TYPE==\"mlp\":\n",
        "    model=MLP(Xtr.shape[1], p=DROPOUT).to(DEVICE)\n",
        "    collate_fn=None\n",
        "else:\n",
        "    model=TrBiLSTM_Sensors(token_dim=len(stat_prefixes), d_model=TR_DIM, nhead=NHEAD, nlay_tr=N_LAY_TR,\n",
        "                           lstm_h=LSTM_H, nlay_lstm=N_LAY_LSTM, p=DROPOUT).to(DEVICE)\n",
        "    train_loader=DataLoader(tr_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=0, collate_fn=pad_collate)\n",
        "    val_loader  =DataLoader(va_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0, collate_fn=pad_collate)\n",
        "    test_loader =DataLoader(te_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0, collate_fn=pad_collate)\n",
        "print(model); print(\"Total params:\", sum(p.numel() for p in model.parameters()))\n",
        "\n",
        "opt=torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=1e-4)\n",
        "crit=nn.BCEWithLogitsLoss()\n",
        "\n",
        "def run_epoch(loader, train=False):\n",
        "    (model.train if train else model.eval)()\n",
        "    total, Ys, Ps = 0.0, [], []\n",
        "    for batch in loader:\n",
        "        if MODEL_TYPE==\"mlp\":\n",
        "            xb, yb = batch[0].to(DEVICE), batch[1].to(DEVICE)\n",
        "        else:\n",
        "            xb, yb = batch\n",
        "        with torch.set_grad_enabled(train):\n",
        "            logit=model(xb)\n",
        "            loss=crit(logit, yb)\n",
        "            if train:\n",
        "                opt.zero_grad(set_to_none=True); loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0); opt.step()\n",
        "        total += loss.item()*len(yb)\n",
        "        Ps.append(torch.sigmoid(logit).detach().cpu().numpy()); Ys.append(yb.detach().cpu().numpy())\n",
        "    y=np.concatenate(Ys); p=np.concatenate(Ps)\n",
        "    pred=(p>=0.5).astype(int)\n",
        "    return total/max(1,len(loader.dataset)), accuracy_score(y,pred), f1_score(y,pred,zero_division=0), y, p\n",
        "\n",
        "def pick_threshold(y, p):\n",
        "    grid=np.unique(np.concatenate([np.linspace(0.01,0.99,49), np.quantile(p, np.linspace(0.01,0.99,49))]))\n",
        "    f1s=[f1_score(y,(p>=t).astype(int),zero_division=0) for t in grid]\n",
        "    i=int(np.argmax(f1s)); return float(grid[i]), float(f1s[i])\n",
        "\n",
        "best_f1, best_thr, patience, noimp = -1.0, 0.5, 3, 0\n",
        "for e in range(1, EPOCHS+1):\n",
        "    t0=time.time()\n",
        "    tr_loss,tr_acc,tr_f1,_,_=run_epoch(train_loader, True)\n",
        "    va_loss,va_acc,va_f1,yv,pv=run_epoch(val_loader, False)\n",
        "    thr_opt,f1_opt=pick_threshold(yv,pv)\n",
        "    print(f\"Epoch {e:02d}/{EPOCHS} | {time.time()-t0:.1f}s | train {tr_loss:.4f}/{tr_acc:.3f}/{tr_f1:.3f} | val f1@0.5 {va_f1:.3f} | best f1 {f1_opt:.3f} @ thr={thr_opt:.2f}\")\n",
        "    if f1_opt>best_f1:\n",
        "        best_f1, best_thr, noimp = f1_opt, thr_opt, 0\n",
        "        torch.save({\"model\":model.state_dict(),\"thr\":best_thr}, CKPT)\n",
        "        print(\"  ✓ Saved:\", CKPT)\n",
        "    else:\n",
        "        noimp += 1\n",
        "        if noimp >= patience:\n",
        "            print(\"Early stopping.\"); break\n",
        "\n",
        "ckpt=torch.load(CKPT, map_location=DEVICE)\n",
        "model.load_state_dict(ckpt[\"model\"]); best_thr=ckpt[\"thr\"]\n",
        "\n",
        "def evaluate(loader, thr):\n",
        "    model.eval(); Ys, Ps = [], []\n",
        "    with torch.no_grad():\n",
        "        for batch in loader:\n",
        "            if MODEL_TYPE==\"mlp\":\n",
        "                xb, yb = batch[0].to(DEVICE), batch[1]\n",
        "            else:\n",
        "                xb, yb = batch; yb = yb.cpu()\n",
        "            pr=torch.sigmoid(model(xb.to(DEVICE))).cpu().numpy()\n",
        "            Ys.append(yb.numpy()); Ps.append(pr)\n",
        "    y=np.concatenate(Ys); p=np.concatenate(Ps)\n",
        "    pred=(p>=thr).astype(int)\n",
        "    acc=accuracy_score(y,pred); f1=f1_score(y,pred,zero_division=0)\n",
        "    return acc, f1, y, pred\n",
        "\n",
        "acc,f1,y_true,y_pred=evaluate(test_loader, best_thr)\n",
        "print(\"\\n--- Test (UNSW-ized records) ---\")\n",
        "print({\"acc\": round(acc,4), \"f1\": round(f1,4), \"thr\": round(best_thr,2)})\n",
        "print(confusion_matrix(y_true,y_pred))\n",
        "print(classification_report(y_true,y_pred, target_names=[\"Normal (0)\",\"Attack (1)\"], zero_division=0))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "emSJJqwq2MhO",
        "outputId": "079531c4-961d-401c-f1d7-6315cd001d32"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n",
            "✓ CSVهای از قبل موجودند.\n",
            "UNSW-ized CSVs: ./wadi_preprocessed/wadi_unswized_train.csv ./wadi_preprocessed/wadi_unswized_val.csv ./wadi_preprocessed/wadi_unswized_test.csv\n",
            "Shapes: (264, 838) (38, 838) (76, 838)\n",
            "Label ratio train/val/test: 0.5 0.5 0.5\n",
            "TrBiLSTM_Sensors(\n",
            "  (proj): Linear(in_features=9, out_features=128, bias=True)\n",
            "  (pos): SinusoidalPositionalEncoding()\n",
            "  (enc): TrEnc(\n",
            "    (layers): ModuleList(\n",
            "      (0-1): 2 x TrEncLayer(\n",
            "        (attn): MultiheadAttention(\n",
            "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
            "        )\n",
            "        (l1): Linear(in_features=128, out_features=256, bias=True)\n",
            "        (l2): Linear(in_features=256, out_features=128, bias=True)\n",
            "        (n1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "        (n2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "        (d1): Dropout(p=0.1, inplace=False)\n",
            "        (d2): Dropout(p=0.1, inplace=False)\n",
            "        (act): GELU(approximate='none')\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (lstm): LSTM(128, 128, batch_first=True, bidirectional=True)\n",
            "  (drop): Dropout(p=0.1, inplace=False)\n",
            "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
            ")\n",
            "Total params: 530689\n",
            "Epoch 01/15 | 0.3s | train 0.6904/0.568/0.610 | val f1@0.5 0.773 | best f1 0.810 @ thr=0.50\n",
            "  ✓ Saved: ./bilstm_trf_checkpoints/best_unswized_trf_bilstm_sensors.pt\n",
            "Epoch 02/15 | 0.3s | train 0.6810/0.648/0.714 | val f1@0.5 0.810 | best f1 0.810 @ thr=0.50\n",
            "Epoch 03/15 | 0.3s | train 0.6744/0.686/0.731 | val f1@0.5 0.780 | best f1 0.810 @ thr=0.49\n",
            "Epoch 04/15 | 0.3s | train 0.6650/0.693/0.731 | val f1@0.5 0.750 | best f1 0.791 @ thr=0.48\n",
            "Early stopping.\n",
            "\n",
            "--- Test (UNSW-ized records) ---\n",
            "{'acc': 0.6447, 'f1': 0.7158, 'thr': 0.5}\n",
            "[[15 23]\n",
            " [ 4 34]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "  Normal (0)       0.79      0.39      0.53        38\n",
            "  Attack (1)       0.60      0.89      0.72        38\n",
            "\n",
            "    accuracy                           0.64        76\n",
            "   macro avg       0.69      0.64      0.62        76\n",
            "weighted avg       0.69      0.64      0.62        76\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, re, sys, math, time, gc, subprocess, warnings\n",
        "from pathlib import Path\n",
        "from typing import Optional, List, Tuple\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix, precision_recall_curve\n",
        "\n",
        "# ===================== Setup =====================\n",
        "SEED = 42\n",
        "np.random.seed(SEED); torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available(): torch.cuda.manual_seed_all(SEED)\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Device:\", DEVICE)\n",
        "\n",
        "# ===================== Config =====================\n",
        "KAGGLE_SLUG = \"giovannimonco/wadi-data\"\n",
        "\n",
        "OUT_DIR_RAW = \"./wadi_preprocessed\"\n",
        "Path(OUT_DIR_RAW).mkdir(parents=True, exist_ok=True)\n",
        "TRAIN_CSV = f\"{OUT_DIR_RAW}/wadi_train_preprocessed.csv\"\n",
        "TEST_CSV  = f\"{OUT_DIR_RAW}/wadi_test_preprocessed.csv\"\n",
        "\n",
        "# UNSW-ization (recordization)\n",
        "SEQ_LEN     = 40\n",
        "STRIDE      = 1\n",
        "DILATE      = 7\n",
        "NEG_RATIO   = 2.0\n",
        "NON_OVERLAP = False\n",
        "HARD_NEG_Q  = 0.90\n",
        "HARD_NEG_Q_TR = 0.90\n",
        "VAL_SIZE = 0.15\n",
        "TEST_SIZE = 0.15\n",
        "\n",
        "# Model/training\n",
        "MODEL_TYPE = \"trf_bilstm_sensors\"  # \"mlp\" or \"trf_bilstm_sensors\"\n",
        "BATCH_SIZE = 512 if DEVICE == \"cuda\" else 256\n",
        "EPOCHS     = 20\n",
        "LR         = 3e-4\n",
        "DROPOUT    = 0.15\n",
        "WEIGHT_DECAY = 3e-4\n",
        "MIN_PREC_AT_TUNE = 0.6\n",
        "EARLY_PATIENCE = 4\n",
        "\n",
        "# Transformer across sensors (token = per-sensor stats)\n",
        "TR_DIM    = 192\n",
        "NHEAD     = 4\n",
        "N_LAY_TR  = 2\n",
        "LSTM_H    = 192\n",
        "N_LAY_LSTM= 1\n",
        "\n",
        "CKPT_DIR = \"./bilstm_trf_checkpoints\"\n",
        "Path(CKPT_DIR).mkdir(parents=True, exist_ok=True)\n",
        "CKPT = f\"{CKPT_DIR}/best_unswized_{MODEL_TYPE}.pt\"\n",
        "\n",
        "# ===================== Preprocess (WADI-like) =====================\n",
        "def strip_and_dedup(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    df.columns = [re.sub(r\"\\s+\",\" \",str(c).strip()) for c in df.columns]\n",
        "    return df.loc[:, ~df.columns.duplicated()]\n",
        "\n",
        "def read_train_csv(p: str) -> pd.DataFrame:\n",
        "    name = Path(p).name.lower()\n",
        "    df = pd.read_csv(p, skiprows=4, low_memory=False) if (\"14days.csv\" in name and \"new\" not in name) else pd.read_csv(p, low_memory=False)\n",
        "    return strip_and_dedup(df)\n",
        "\n",
        "def read_test_csv(p: str) -> pd.DataFrame:\n",
        "    name = Path(p).name.lower()\n",
        "    try:\n",
        "        df = pd.read_csv(p, skiprows=1, low_memory=False) if \"lable\" in name else pd.read_csv(p, low_memory=False)\n",
        "    except Exception:\n",
        "        df = pd.read_csv(p, low_memory=False)\n",
        "    df = strip_and_dedup(df)\n",
        "    if df.iloc[0].astype(str).str.contains(\"Row|Date|Time|Attack\", case=False, regex=True).any():\n",
        "        df.columns = df.iloc[0].astype(str).tolist()\n",
        "        df = strip_and_dedup(df.iloc[1:].reset_index(drop=True))\n",
        "    return df\n",
        "\n",
        "def extract_label_from_test(df: pd.DataFrame) -> pd.Series:\n",
        "    cands = [c for c in df.columns if re.search(r\"(label|lable|attack)\", c, flags=re.I)]\n",
        "    lab = None\n",
        "    for c in cands:\n",
        "        col = pd.to_numeric(df[c], errors=\"coerce\")\n",
        "        u = set(col.dropna().unique().tolist())\n",
        "        if u.issubset({-1,0,1}) and len(u)>0:\n",
        "            lab = col.map({1:0, -1:1, 0:0}); break\n",
        "    if lab is None: lab = pd.Series(np.zeros(len(df), dtype=np.int64))\n",
        "    return lab.astype(np.int64).rename(\"label\")\n",
        "\n",
        "def numeric_common(train_df: pd.DataFrame, test_df: pd.DataFrame):\n",
        "    drop = {\"Date\",\"Time\",\"datetime\",\"timestamp\",\"Row\",\"row\",\"index\"}\n",
        "    num_tr = [c for c in train_df.columns if pd.api.types.is_numeric_dtype(train_df[c]) and c not in drop]\n",
        "    num_te = [c for c in test_df.columns  if pd.api.types.is_numeric_dtype(test_df[c]) and c not in drop]\n",
        "    feats = sorted(list(set(num_tr) & set(num_te)))\n",
        "    if not feats: raise RuntimeError(\"No shared numeric columns.\")\n",
        "    Xtr = train_df[feats].copy(); Xte = test_df[feats].copy()\n",
        "    for X in (Xtr, Xte):\n",
        "        X.replace([\"?\",\"NA\",\"NaN\",\"nan\",\"\"], np.nan, inplace=True)\n",
        "        for c in X.columns: X[c] = pd.to_numeric(X[c], errors=\"coerce\")\n",
        "        X.interpolate(limit_direction=\"both\", inplace=True)\n",
        "        X.ffill(inplace=True); X.bfill(inplace=True)\n",
        "    return Xtr, Xte, feats\n",
        "\n",
        "def safe_scale(Xtr: pd.DataFrame, Xte: pd.DataFrame, eps: float=1e-8):\n",
        "    std = Xtr.std(0, ddof=0)\n",
        "    keep = std[std > eps].index.tolist()\n",
        "    if len(keep) < len(std): print(f\"[info] dropped {len(std)-len(keep)} low-variance cols\")\n",
        "    mu = Xtr[keep].mean(0); sd = Xtr[keep].std(0, ddof=0).clip(lower=eps)\n",
        "    Xtr = ((Xtr[keep]-mu)/sd).astype(np.float32).replace([np.inf,-np.inf], np.nan).fillna(0.0)\n",
        "    Xte = ((Xte[keep]-mu)/sd).astype(np.float32).replace([np.inf,-np.inf], np.nan).fillna(0.0)\n",
        "    return Xtr, Xte, keep\n",
        "\n",
        "def find_file(cands: List[str], root: str) -> Optional[str]:\n",
        "    low = [c.lower() for c in cands]\n",
        "    for c in cands:\n",
        "        p = Path(root)/c\n",
        "        if p.exists(): return str(p)\n",
        "    for r,_,files in os.walk(root):\n",
        "        for f in files:\n",
        "            if f.lower() in low: return str(Path(r)/f)\n",
        "    for r,_,files in os.walk(root):\n",
        "        for f in files:\n",
        "            fl = f.lower()\n",
        "            for cand in low:\n",
        "                if cand.replace(\"_\",\"\").replace(\"-\",\"\") in fl.replace(\"_\",\"\").replace(\"-\",\"\"):\n",
        "                    return str(Path(r)/f)\n",
        "    return None\n",
        "\n",
        "def download_or_cache() -> str:\n",
        "    try:\n",
        "        import kagglehub\n",
        "    except Exception:\n",
        "        subprocess.check_call([sys.executable,\"-m\",\"pip\",\"install\",\"-q\",\"kagglehub\"]); import kagglehub\n",
        "    print(\"➜ kagglehub ...\")\n",
        "    p = kagglehub.dataset_download(KAGGLE_SLUG)\n",
        "    print(\"[ok]\", p); return p\n",
        "\n",
        "def ensure_preprocessed():\n",
        "    if os.path.exists(TEST_CSV) and os.path.exists(TRAIN_CSV):\n",
        "        print(\"✓ CSVهای از قبل موجودند.\")\n",
        "        return\n",
        "    print(\"➜ دانلود WADI ...\")\n",
        "    root = download_or_cache()\n",
        "    tr = find_file([\"WADI_14days_new.csv\",\"WADI_14days.csv\"], root)\n",
        "    te = find_file([\"WADI_attackdataLABLE.csv\",\"WADI_attackdata.csv\"], root)\n",
        "    if not tr or not te: raise FileNotFoundError(\"WADI CSVs not found\")\n",
        "    train_df = read_train_csv(tr); test_df = read_test_csv(te)\n",
        "    ytest = extract_label_from_test(test_df)\n",
        "    Xtr_raw, Xte_raw, feats = numeric_common(train_df, test_df)\n",
        "    Xtr, Xte, kept = safe_scale(Xtr_raw, Xte_raw)\n",
        "    tr_out = Xtr.copy(); tr_out[\"label\"] = 0\n",
        "    te_out = Xte.copy(); te_out[\"label\"] = ytest.values\n",
        "    tr_out.to_csv(TRAIN_CSV, index=False); te_out.to_csv(TEST_CSV, index=False)\n",
        "    print(\"✓ ساخته شد:\", TRAIN_CSV, TEST_CSV, \"| n_feats:\", len(kept))\n",
        "\n",
        "ensure_preprocessed()\n",
        "\n",
        "# ===================== UNSW-ize (record extraction) =====================\n",
        "df_attack = pd.read_csv(TEST_CSV)\n",
        "FEATURES = [c for c in df_attack.columns if c != \"label\"]\n",
        "X_attack = df_attack[FEATURES].astype(np.float32).values\n",
        "y_attack = df_attack[\"label\"].astype(np.int64).values\n",
        "del df_attack; gc.collect()\n",
        "\n",
        "if DILATE > 0:\n",
        "    k = np.ones(2*DILATE+1, dtype=np.int32)\n",
        "    y_attack = (np.convolve(y_attack, k, mode=\"same\") > 0).astype(np.int64)\n",
        "\n",
        "n_seq_att = len(X_attack) - SEQ_LEN + 1\n",
        "starts_att = np.arange(0, n_seq_att, STRIDE, dtype=np.int64)\n",
        "\n",
        "from numpy.lib.stride_tricks import sliding_window_view\n",
        "y_win_full_att = (sliding_window_view(y_attack, SEQ_LEN).max(axis=1)).astype(np.int64)\n",
        "y_sel_att = y_win_full_att[starts_att]\n",
        "pos_idx_att = starts_att[y_sel_att==1].copy()\n",
        "neg_idx_att = starts_att[y_sel_att==0].copy()\n",
        "\n",
        "diffs = np.abs(np.diff(X_attack, axis=0))\n",
        "act = diffs.mean(axis=1)\n",
        "act_win = sliding_window_view(act, SEQ_LEN-1).mean(axis=1)\n",
        "scores_att = act_win[starts_att]\n",
        "thr_att = np.quantile(scores_att[y_sel_att==0], HARD_NEG_Q)\n",
        "hard_neg_att = neg_idx_att[scores_att[y_sel_att==0] >= thr_att]\n",
        "\n",
        "n_pos = len(pos_idx_att)\n",
        "if n_pos == 0:\n",
        "    raise RuntimeError(\"No positive windows; decrease SEQ_LEN or increase DILATE.\")\n",
        "n_neg = int(max(1, math.ceil(NEG_RATIO * n_pos)))\n",
        "\n",
        "if len(hard_neg_att) >= n_neg:\n",
        "    sel_neg_att = np.random.choice(hard_neg_att, size=n_neg, replace=False)\n",
        "else:\n",
        "    rest_att = np.setdiff1d(neg_idx_att, hard_neg_att)\n",
        "    add_att = np.random.choice(rest_att, size=max(0, n_neg-len(hard_neg_att)), replace=False)\n",
        "    sel_neg_att = np.concatenate([hard_neg_att, add_att]) if len(hard_neg_att)>0 else add_att\n",
        "\n",
        "def make_non_overlapping(idxs: np.ndarray) -> np.ndarray:\n",
        "    if not NON_OVERLAP: return idxs\n",
        "    idxs = np.sort(idxs)\n",
        "    picked, last_end = [], -1\n",
        "    for s in idxs:\n",
        "        if s >= last_end:\n",
        "            picked.append(s); last_end = s + SEQ_LEN\n",
        "    return np.array(picked, dtype=np.int64)\n",
        "\n",
        "pos_idx_att = make_non_overlapping(pos_idx_att)\n",
        "sel_neg_att = make_non_overlapping(sel_neg_att)\n",
        "\n",
        "# negatives from 14-day normal\n",
        "df_trnorm = pd.read_csv(TRAIN_CSV)\n",
        "X_trn = df_trnorm.drop(columns=[\"label\"], errors=\"ignore\").astype(np.float32).values\n",
        "n_seq_tr = len(X_trn) - SEQ_LEN + 1\n",
        "starts_tr = np.arange(0, n_seq_tr, STRIDE, dtype=np.int64)\n",
        "diffs_tr = np.abs(np.diff(X_trn, axis=0))\n",
        "act_tr = diffs_tr.mean(axis=1)\n",
        "act_win_tr = sliding_window_view(act_tr, SEQ_LEN-1).mean(axis=1)\n",
        "scores_tr = act_win_tr[starts_tr]\n",
        "thr_tr = np.quantile(scores_tr, HARD_NEG_Q_TR)\n",
        "hard_neg_tr = starts_tr[scores_tr >= thr_tr]\n",
        "take_tr = int(0.3 * len(sel_neg_att))\n",
        "take_tr = min(take_tr, len(hard_neg_tr))\n",
        "sel_neg_tr = np.random.choice(hard_neg_tr, size=take_tr, replace=False) if take_tr>0 else np.array([], dtype=np.int64)\n",
        "\n",
        "def aggregate_window(block: np.ndarray) -> np.ndarray:\n",
        "    mean = block.mean(0)\n",
        "    std  = block.std(0, ddof=0)\n",
        "    minv = block.min(0)\n",
        "    maxv = block.max(0)\n",
        "    last = block[-1]\n",
        "    delta= last - block[0]\n",
        "    t = np.arange(len(block))\n",
        "    slope = np.polyfit(t, block, deg=1)[0]\n",
        "    q75 = np.quantile(block, 0.75, axis=0); q25 = np.quantile(block, 0.25, axis=0)\n",
        "    iqr = q75 - q25\n",
        "    dsum = np.abs(np.diff(block, axis=0)).sum(0)\n",
        "    return np.concatenate([mean, std, minv, maxv, last, delta, slope, iqr, dsum], axis=0)\n",
        "\n",
        "stat_prefixes = [\"mean\",\"std\",\"min\",\"max\",\"last\",\"delta\",\"slope\",\"iqr\",\"dsum\"]\n",
        "\n",
        "blocks_pos = [aggregate_window(X_attack[s:s+SEQ_LEN,:]) for s in pos_idx_att]\n",
        "y_pos = np.ones(len(blocks_pos), dtype=np.int64)\n",
        "\n",
        "blocks_neg_att = [aggregate_window(X_attack[s:s+SEQ_LEN,:]) for s in sel_neg_att]\n",
        "y_neg_att = np.zeros(len(blocks_neg_att), dtype=np.int64)\n",
        "\n",
        "blocks_neg_tr = [aggregate_window(X_trn[s:s+SEQ_LEN,:]) for s in sel_neg_tr]\n",
        "y_neg_tr = np.zeros(len(blocks_neg_tr), dtype=np.int64)\n",
        "\n",
        "X_agg = np.vstack([np.stack(blocks_pos), np.stack(blocks_neg_att)] + ([np.stack(blocks_neg_tr)] if len(blocks_neg_tr)>0 else []))\n",
        "y_agg = np.concatenate([y_pos, y_neg_att] + ([y_neg_tr] if len(y_neg_tr)>0 else []))\n",
        "\n",
        "cols = []\n",
        "for pfx in stat_prefixes:\n",
        "    cols += [f\"{pfx}_{f}\" for f in FEATURES]\n",
        "df_out = pd.DataFrame(X_agg, columns=cols)\n",
        "df_out[\"label\"] = y_agg\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "tmp_size = VAL_SIZE + TEST_SIZE\n",
        "train_df, tmp_df = train_test_split(df_out, test_size=tmp_size, random_state=SEED, stratify=df_out[\"label\"])\n",
        "rel_test = TEST_SIZE / (VAL_SIZE + TEST_SIZE)\n",
        "val_df, test_df = train_test_split(tmp_df, test_size=rel_test, random_state=SEED, stratify=tmp_df[\"label\"])\n",
        "\n",
        "UNSW_TRAIN = f\"{OUT_DIR_RAW}/wadi_unswized_train.csv\"\n",
        "UNSW_VAL   = f\"{OUT_DIR_RAW}/wadi_unswized_val.csv\"\n",
        "UNSW_TEST  = f\"{OUT_DIR_RAW}/wadi_unswized_test.csv\"\n",
        "train_df.to_csv(UNSW_TRAIN, index=False)\n",
        "val_df.to_csv(UNSW_VAL, index=False)\n",
        "test_df.to_csv(UNSW_TEST, index=False)\n",
        "\n",
        "print(\"UNSW-ized CSVs:\", UNSW_TRAIN, UNSW_VAL, UNSW_TEST)\n",
        "print(\"Shapes:\", train_df.shape, val_df.shape, test_df.shape)\n",
        "print(\"Label ratio train/val/test:\", round(train_df[\"label\"].mean(),3), round(val_df[\"label\"].mean(),3), round(test_df[\"label\"].mean(),3))\n",
        "\n",
        "# ===================== Scaling =====================\n",
        "all_cols = [c for c in train_df.columns if c != \"label\"]\n",
        "Xtr = train_df[all_cols].values.astype(np.float32)\n",
        "Xva = val_df[all_cols].values.astype(np.float32)\n",
        "Xte = test_df[all_cols].values.astype(np.float32)\n",
        "ytr = train_df[\"label\"].values.astype(np.float32)\n",
        "yva = val_df[\"label\"].values.astype(np.float32)\n",
        "yte = test_df[\"label\"].values.astype(np.float32)\n",
        "\n",
        "mu = Xtr.mean(0); sd = Xtr.std(0, ddof=0).clip(min=1e-8)\n",
        "Xtr = ((Xtr - mu)/sd).astype(np.float32)\n",
        "Xva = ((Xva - mu)/sd).astype(np.float32)\n",
        "Xte = ((Xte - mu)/sd).astype(np.float32)\n",
        "Xtr = np.nan_to_num(Xtr, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "Xva = np.nan_to_num(Xva, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "Xte = np.nan_to_num(Xte, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "\n",
        "# ===================== Datasets =====================\n",
        "sensor_names = FEATURES[:]  # original feature (sensor) names\n",
        "PFX = stat_prefixes\n",
        "token_dim = len(PFX)\n",
        "\n",
        "# build fixed index map: for every sensor, indices of its stats in all_cols\n",
        "sensor_to_idx = {}\n",
        "for feat in sensor_names:\n",
        "    idxs = []\n",
        "    ok = True\n",
        "    for p in PFX:\n",
        "        col = f\"{p}_{feat}\"\n",
        "        if col not in all_cols:\n",
        "            ok = False; break\n",
        "        idxs.append(all_cols.index(col))\n",
        "    if ok:\n",
        "        sensor_to_idx[feat] = idxs\n",
        "sensor_order = [s for s in sensor_names if s in sensor_to_idx]\n",
        "S_eff = len(sensor_order)\n",
        "sensor_index_matrix = np.array([sensor_to_idx[s] for s in sensor_order], dtype=np.int64)  # [S_eff, token_dim]\n",
        "\n",
        "def row_to_tokens(row_vec: np.ndarray) -> np.ndarray:\n",
        "    return row_vec[sensor_index_matrix]  # [S_eff, token_dim]\n",
        "\n",
        "class DSMLP(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = torch.from_numpy(X)\n",
        "        self.y = torch.from_numpy(y)\n",
        "    def __len__(self): return len(self.y)\n",
        "    def __getitem__(self,i): return self.X[i], self.y[i]\n",
        "\n",
        "class DSSensors(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        toks = np.stack([row_to_tokens(X[i]) for i in range(len(X))], axis=0)  # [N,S_eff,token_dim]\n",
        "        self.X = torch.from_numpy(toks.astype(np.float32))\n",
        "        self.y = torch.from_numpy(y.astype(np.float32))\n",
        "    def __len__(self): return len(self.y)\n",
        "    def __getitem__(self,i): return self.X[i], self.y[i]\n",
        "\n",
        "if MODEL_TYPE == \"mlp\":\n",
        "    tr_ds = DSMLP(Xtr, ytr); va_ds = DSMLP(Xva, yva); te_ds = DSMLP(Xte, yte)\n",
        "else:\n",
        "    tr_ds = DSSensors(Xtr, ytr); va_ds = DSSensors(Xva, yva); te_ds = DSSensors(Xte, yte)\n",
        "\n",
        "train_loader = DataLoader(tr_ds, batch_size=BATCH_SIZE, shuffle=True,  num_workers=0)\n",
        "val_loader   = DataLoader(va_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
        "test_loader  = DataLoader(te_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
        "\n",
        "# ===================== Models =====================\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, in_dim, p=0.2):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(in_dim, 512), nn.ReLU(), nn.Dropout(p),\n",
        "            nn.Linear(512, 256), nn.ReLU(), nn.Dropout(p),\n",
        "            nn.Linear(256, 1)\n",
        "        )\n",
        "    def forward(self, x): return self.net(x).squeeze(1)\n",
        "\n",
        "class SinusoidalPositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=4096):\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, d_model, dtype=torch.float32)\n",
        "        pos = torch.arange(0, max_len, dtype=torch.float32).unsqueeze(1)\n",
        "        div = torch.exp(torch.arange(0, d_model, 2, dtype=torch.float32) * (-math.log(10000.0)/d_model))\n",
        "        pe[:,0::2] = torch.sin(pos*div); pe[:,1::2] = torch.cos(pos*div)\n",
        "        self.register_buffer(\"pe\", pe.unsqueeze(0))\n",
        "    def forward(self, x): return x + self.pe[:, :x.size(1), :]\n",
        "\n",
        "class TrEncLayer(nn.Module):\n",
        "    def __init__(self, d, nhead, ff, p):\n",
        "        super().__init__()\n",
        "        self.attn = nn.MultiheadAttention(d, nhead, dropout=p, batch_first=True)\n",
        "        self.l1, self.l2 = nn.Linear(d, ff), nn.Linear(ff, d)\n",
        "        self.n1, self.n2 = nn.LayerNorm(d), nn.LayerNorm(d)\n",
        "        self.d1, self.d2 = nn.Dropout(p), nn.Dropout(p)\n",
        "        self.act = nn.GELU()\n",
        "    def forward(self, x):\n",
        "        a,_ = self.attn(x,x,x,need_weights=False)\n",
        "        x = self.n1(x + self.d1(a))\n",
        "        x = self.n2(x + self.d2(self.l2(self.d1(self.act(self.l1(x))))))\n",
        "        return x\n",
        "\n",
        "class TrEnc(nn.Module):\n",
        "    def __init__(self, d, nhead, nlayer, ff, p):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([TrEncLayer(d, nhead, ff, p) for _ in range(nlayer)])\n",
        "    def forward(self, x):\n",
        "        for lyr in self.layers: x = lyr(x)\n",
        "        return x\n",
        "\n",
        "class TrBiLSTM_Sensors_Att(nn.Module):\n",
        "    def __init__(self, token_dim, d_model, nhead, nlay_tr, lstm_h, nlay_lstm, p=0.1):\n",
        "        super().__init__()\n",
        "        self.proj = nn.Linear(token_dim, d_model)\n",
        "        self.pos  = SinusoidalPositionalEncoding(d_model)\n",
        "        self.enc  = TrEnc(d_model, nhead, nlay_tr, d_model*2, p)\n",
        "        self.lstm = nn.LSTM(d_model, lstm_h, num_layers=nlay_lstm, batch_first=True,\n",
        "                            bidirectional=True, dropout=p if nlay_lstm>1 else 0.0)\n",
        "        self.drop = nn.Dropout(p)\n",
        "        self.att  = nn.Sequential(\n",
        "            nn.Linear(lstm_h*2, d_model//2),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(d_model//2, 1)\n",
        "        )\n",
        "        self.fc   = nn.Linear(lstm_h*2, 1)\n",
        "    def forward(self, x):              # x: [B,S,token_dim]\n",
        "        x = self.proj(x)               # [B,S,D]\n",
        "        x = self.pos(x)\n",
        "        x = self.enc(x)\n",
        "        x,_ = self.lstm(x)             # [B,S,2H]\n",
        "        x = self.drop(x)\n",
        "        w = torch.softmax(self.att(x).squeeze(-1), dim=1)  # [B,S]\n",
        "        x = (x * w.unsqueeze(-1)).sum(1)                   # [B,2H]\n",
        "        return self.fc(x).squeeze(1)\n",
        "\n",
        "if MODEL_TYPE == \"mlp\":\n",
        "    model = MLP(Xtr.shape[1], p=DROPOUT).to(DEVICE)\n",
        "else:\n",
        "    model = TrBiLSTM_Sensors_Att(token_dim=token_dim, d_model=TR_DIM, nhead=NHEAD, nlay_tr=N_LAY_TR,\n",
        "                                 lstm_h=LSTM_H, nlay_lstm=N_LAY_LSTM, p=DROPOUT).to(DEVICE)\n",
        "\n",
        "print(model)\n",
        "print(\"Total params:\", sum(p.numel() for p in model.parameters()))\n",
        "\n",
        "# ===================== Loss/Opt =====================\n",
        "class FocalBCEWithLogitsLoss(nn.Module):\n",
        "    def __init__(self, alpha=0.6, gamma=2.0):\n",
        "        super().__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.bce = nn.BCEWithLogitsLoss(reduction=\"none\")\n",
        "    def forward(self, logits, targets):\n",
        "        loss_bce = self.bce(logits, targets)\n",
        "        p = torch.sigmoid(logits)\n",
        "        pt = torch.where(targets==1.0, p, 1.0-p)\n",
        "        mod = (1.0 - pt).pow(self.gamma)\n",
        "        loss = self.alpha * mod * loss_bce\n",
        "        return loss.mean()\n",
        "\n",
        "crit = FocalBCEWithLogitsLoss(alpha=0.6, gamma=2.0)\n",
        "opt  = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
        "\n",
        "def run_epoch(loader, train=False):\n",
        "    (model.train if train else model.eval)()\n",
        "    total, Ys, Ps = 0.0, [], []\n",
        "    for xb, yb in loader:\n",
        "        xb = xb.to(DEVICE); yb = yb.to(DEVICE)\n",
        "        with torch.set_grad_enabled(train):\n",
        "            logit = model(xb)\n",
        "            loss = crit(logit, yb)\n",
        "            if train:\n",
        "                opt.zero_grad(set_to_none=True); loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "                opt.step()\n",
        "        total += loss.item() * len(yb)\n",
        "        Ps.append(torch.sigmoid(logit).detach().cpu().numpy())\n",
        "        Ys.append(yb.detach().cpu().numpy())\n",
        "    y = np.concatenate(Ys) if Ys else np.array([])\n",
        "    p = np.concatenate(Ps) if Ps else np.array([])\n",
        "    pred = (p >= 0.5).astype(int) if len(p) else np.array([])\n",
        "    acc = accuracy_score(y, pred) if len(y) else 0.0\n",
        "    f1  = f1_score(y, pred, zero_division=0) if len(y) else 0.0\n",
        "    return total/max(1,len(loader.dataset)), acc, f1, y, p\n",
        "\n",
        "def pick_threshold_constrained(y, p, min_prec=0.6):\n",
        "    prec, rec, thr = precision_recall_curve(y, p)\n",
        "    best_t, best_f1 = None, -1.0\n",
        "    for pr, rc, t in zip(prec[:-1], rec[:-1], thr):\n",
        "        if pr >= min_prec:\n",
        "            f1 = 0.0 if (pr+rc)==0 else 2*pr*rc/(pr+rc)\n",
        "            if f1 > best_f1:\n",
        "                best_f1, best_t = f1, float(t)\n",
        "    if best_t is None:\n",
        "        qs = np.quantile(p, np.linspace(0.4,0.9,6))\n",
        "        f1s = [f1_score(y, (p>=t).astype(int), zero_division=0) for t in qs]\n",
        "        best_t = float(qs[int(np.argmax(f1s))])\n",
        "    return best_t, float(f1_score(y, (p>=best_t).astype(int), zero_division=0))\n",
        "\n",
        "best_f1, best_thr, patience, noimp = -1.0, 0.5, EARLY_PATIENCE, 0\n",
        "for e in range(1, EPOCHS+1):\n",
        "    t0 = time.time()\n",
        "    tr_loss, tr_acc, tr_f1, _, _ = run_epoch(train_loader, True)\n",
        "    va_loss, va_acc, va_f1, yv, pv = run_epoch(val_loader, False)\n",
        "    thr_opt, f1_opt = pick_threshold_constrained(yv, pv, min_prec=MIN_PREC_AT_TUNE)\n",
        "    print(f\"Epoch {e:02d}/{EPOCHS} | {time.time()-t0:.1f}s | train {tr_loss:.4f}/{tr_acc:.3f}/{tr_f1:.3f} | val f1@0.5 {va_f1:.3f} | best f1 {f1_opt:.3f} @ thr={thr_opt:.2f}\")\n",
        "    if f1_opt > best_f1:\n",
        "        best_f1, best_thr, noimp = f1_opt, thr_opt, 0\n",
        "        torch.save({\"model\": model.state_dict(), \"thr\": best_thr}, CKPT)\n",
        "        print(\"  ✓ Saved:\", CKPT)\n",
        "    else:\n",
        "        noimp += 1\n",
        "        if noimp >= patience:\n",
        "            print(\"Early stopping.\"); break\n",
        "\n",
        "ckpt = torch.load(CKPT, map_location=DEVICE)\n",
        "model.load_state_dict(ckpt[\"model\"]); best_thr = ckpt[\"thr\"]\n",
        "\n",
        "def evaluate(loader, thr):\n",
        "    model.eval(); Ys, Ps = [], []\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in loader:\n",
        "            xb = xb.to(DEVICE)\n",
        "            pr = torch.sigmoid(model(xb)).cpu().numpy()\n",
        "            Ys.append(yb.numpy()); Ps.append(pr)\n",
        "    y = np.concatenate(Ys); p = np.concatenate(Ps)\n",
        "    pred = (p >= thr).astype(int)\n",
        "    acc = accuracy_score(y, pred); f1 = f1_score(y, pred, zero_division=0)\n",
        "    return acc, f1, y, pred\n",
        "\n",
        "acc, f1, y_true, y_pred = evaluate(test_loader, best_thr)\n",
        "print(\"\\n--- Test (UNSW-ized records) ---\")\n",
        "print({\"acc\": round(acc,4), \"f1\": round(f1,4), \"thr\": round(best_thr,3)})\n",
        "print(confusion_matrix(y_true, y_pred))\n",
        "print(classification_report(y_true, y_pred, target_names=[\"Normal (0)\",\"Attack (1)\"], zero_division=0))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "57YN4jYj5OmT",
        "outputId": "0fef410e-9304-4e4f-a48e-cda0d17f8bb9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n",
            "✓ CSVهای از قبل موجودند.\n",
            "UNSW-ized CSVs: ./wadi_preprocessed/wadi_unswized_train.csv ./wadi_preprocessed/wadi_unswized_val.csv ./wadi_preprocessed/wadi_unswized_test.csv\n",
            "Shapes: (27011, 838) (5788, 838) (5789, 838)\n",
            "Label ratio train/val/test: 0.278 0.278 0.278\n",
            "TrBiLSTM_Sensors_Att(\n",
            "  (proj): Linear(in_features=9, out_features=192, bias=True)\n",
            "  (pos): SinusoidalPositionalEncoding()\n",
            "  (enc): TrEnc(\n",
            "    (layers): ModuleList(\n",
            "      (0-1): 2 x TrEncLayer(\n",
            "        (attn): MultiheadAttention(\n",
            "          (out_proj): NonDynamicallyQuantizableLinear(in_features=192, out_features=192, bias=True)\n",
            "        )\n",
            "        (l1): Linear(in_features=192, out_features=384, bias=True)\n",
            "        (l2): Linear(in_features=384, out_features=192, bias=True)\n",
            "        (n1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
            "        (n2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
            "        (d1): Dropout(p=0.15, inplace=False)\n",
            "        (d2): Dropout(p=0.15, inplace=False)\n",
            "        (act): GELU(approximate='none')\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (lstm): LSTM(192, 192, batch_first=True, bidirectional=True)\n",
            "  (drop): Dropout(p=0.15, inplace=False)\n",
            "  (att): Sequential(\n",
            "    (0): Linear(in_features=384, out_features=96, bias=True)\n",
            "    (1): GELU(approximate='none')\n",
            "    (2): Linear(in_features=96, out_features=1, bias=True)\n",
            "  )\n",
            "  (fc): Linear(in_features=384, out_features=1, bias=True)\n",
            ")\n",
            "Total params: 1226306\n",
            "Epoch 01/20 | 23.7s | train 0.0765/0.754/0.382 | val f1@0.5 0.602 | best f1 0.700 @ thr=0.38\n",
            "  ✓ Saved: ./bilstm_trf_checkpoints/best_unswized_trf_bilstm_sensors.pt\n",
            "Epoch 02/20 | 24.0s | train 0.0456/0.872/0.752 | val f1@0.5 0.865 | best f1 0.882 @ thr=0.39\n",
            "  ✓ Saved: ./bilstm_trf_checkpoints/best_unswized_trf_bilstm_sensors.pt\n",
            "Epoch 03/20 | 23.6s | train 0.0218/0.944/0.898 | val f1@0.5 0.922 | best f1 0.953 @ thr=0.63\n",
            "  ✓ Saved: ./bilstm_trf_checkpoints/best_unswized_trf_bilstm_sensors.pt\n",
            "Epoch 04/20 | 23.5s | train 0.0128/0.970/0.946 | val f1@0.5 0.970 | best f1 0.974 @ thr=0.43\n",
            "  ✓ Saved: ./bilstm_trf_checkpoints/best_unswized_trf_bilstm_sensors.pt\n",
            "Epoch 05/20 | 23.6s | train 0.0097/0.977/0.959 | val f1@0.5 0.980 | best f1 0.981 @ thr=0.55\n",
            "  ✓ Saved: ./bilstm_trf_checkpoints/best_unswized_trf_bilstm_sensors.pt\n",
            "Epoch 06/20 | 23.7s | train 0.0056/0.988/0.978 | val f1@0.5 0.984 | best f1 0.986 @ thr=0.66\n",
            "  ✓ Saved: ./bilstm_trf_checkpoints/best_unswized_trf_bilstm_sensors.pt\n",
            "Epoch 07/20 | 23.8s | train 0.0052/0.987/0.977 | val f1@0.5 0.989 | best f1 0.990 @ thr=0.54\n",
            "  ✓ Saved: ./bilstm_trf_checkpoints/best_unswized_trf_bilstm_sensors.pt\n",
            "Epoch 08/20 | 23.5s | train 0.0031/0.993/0.987 | val f1@0.5 0.990 | best f1 0.991 @ thr=0.38\n",
            "  ✓ Saved: ./bilstm_trf_checkpoints/best_unswized_trf_bilstm_sensors.pt\n",
            "Epoch 09/20 | 23.6s | train 0.0034/0.992/0.986 | val f1@0.5 0.984 | best f1 0.991 @ thr=0.64\n",
            "Epoch 10/20 | 23.6s | train 0.0027/0.994/0.989 | val f1@0.5 0.993 | best f1 0.994 @ thr=0.43\n",
            "  ✓ Saved: ./bilstm_trf_checkpoints/best_unswized_trf_bilstm_sensors.pt\n",
            "Epoch 11/20 | 23.6s | train 0.0025/0.994/0.990 | val f1@0.5 0.988 | best f1 0.991 @ thr=0.28\n",
            "Epoch 12/20 | 23.6s | train 0.0025/0.994/0.989 | val f1@0.5 0.993 | best f1 0.996 @ thr=0.61\n",
            "  ✓ Saved: ./bilstm_trf_checkpoints/best_unswized_trf_bilstm_sensors.pt\n",
            "Epoch 13/20 | 23.7s | train 0.0020/0.996/0.993 | val f1@0.5 0.988 | best f1 0.993 @ thr=0.28\n",
            "Epoch 14/20 | 23.5s | train 0.0025/0.994/0.990 | val f1@0.5 0.987 | best f1 0.988 @ thr=0.41\n",
            "Epoch 15/20 | 23.6s | train 0.0024/0.995/0.991 | val f1@0.5 0.991 | best f1 0.995 @ thr=0.60\n",
            "Epoch 16/20 | 23.6s | train 0.0022/0.995/0.991 | val f1@0.5 0.993 | best f1 0.993 @ thr=0.51\n",
            "Early stopping.\n",
            "\n",
            "--- Test (UNSW-ized records) ---\n",
            "{'acc': 0.9978, 'f1': 0.996, 'thr': 0.611}\n",
            "[[4169   12]\n",
            " [   1 1607]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "  Normal (0)       1.00      1.00      1.00      4181\n",
            "  Attack (1)       0.99      1.00      1.00      1608\n",
            "\n",
            "    accuracy                           1.00      5789\n",
            "   macro avg       1.00      1.00      1.00      5789\n",
            "weighted avg       1.00      1.00      1.00      5789\n",
            "\n"
          ]
        }
      ]
    }
  ]
}