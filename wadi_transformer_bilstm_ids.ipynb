{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rezahamzeh69/wadi_transformer_bilstm_ids/blob/main/wadi_transformer_bilstm_ids.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, re, sys, math, time, gc, subprocess, warnings\n",
        "from pathlib import Path\n",
        "from typing import Optional, List, Tuple\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix, precision_recall_curve\n",
        "\n",
        "# ===================== Setup =====================\n",
        "SEED = 42\n",
        "np.random.seed(SEED); torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available(): torch.cuda.manual_seed_all(SEED)\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Device:\", DEVICE)\n",
        "\n",
        "# ===================== Config =====================\n",
        "KAGGLE_SLUG = \"giovannimonco/wadi-data\"\n",
        "\n",
        "OUT_DIR_RAW = \"./wadi_preprocessed\"\n",
        "Path(OUT_DIR_RAW).mkdir(parents=True, exist_ok=True)\n",
        "TRAIN_CSV = f\"{OUT_DIR_RAW}/wadi_train_preprocessed.csv\"\n",
        "TEST_CSV  = f\"{OUT_DIR_RAW}/wadi_test_preprocessed.csv\"\n",
        "\n",
        "# UNSW-ization (recordization)\n",
        "SEQ_LEN     = 40\n",
        "STRIDE      = 1\n",
        "DILATE      = 7\n",
        "NEG_RATIO   = 2.0\n",
        "NON_OVERLAP = False\n",
        "HARD_NEG_Q  = 0.90\n",
        "HARD_NEG_Q_TR = 0.90\n",
        "VAL_SIZE = 0.15\n",
        "TEST_SIZE = 0.15\n",
        "\n",
        "# Model/training\n",
        "MODEL_TYPE = \"trf_bilstm_sensors\"  # \"mlp\" or \"trf_bilstm_sensors\"\n",
        "BATCH_SIZE = 512 if DEVICE == \"cuda\" else 256\n",
        "EPOCHS     = 20\n",
        "LR         = 3e-4\n",
        "DROPOUT    = 0.15\n",
        "WEIGHT_DECAY = 3e-4\n",
        "MIN_PREC_AT_TUNE = 0.6\n",
        "EARLY_PATIENCE = 4\n",
        "\n",
        "# Transformer across sensors (token = per-sensor stats)\n",
        "TR_DIM    = 192\n",
        "NHEAD     = 4\n",
        "N_LAY_TR  = 2\n",
        "LSTM_H    = 192\n",
        "N_LAY_LSTM= 1\n",
        "\n",
        "CKPT_DIR = \"./bilstm_trf_checkpoints\"\n",
        "Path(CKPT_DIR).mkdir(parents=True, exist_ok=True)\n",
        "CKPT = f\"{CKPT_DIR}/best_unswized_{MODEL_TYPE}.pt\"\n",
        "\n",
        "# ===================== Preprocess (WADI-like) =====================\n",
        "def strip_and_dedup(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    df.columns = [re.sub(r\"\\s+\",\" \",str(c).strip()) for c in df.columns]\n",
        "    return df.loc[:, ~df.columns.duplicated()]\n",
        "\n",
        "def read_train_csv(p: str) -> pd.DataFrame:\n",
        "    name = Path(p).name.lower()\n",
        "    df = pd.read_csv(p, skiprows=4, low_memory=False) if (\"14days.csv\" in name and \"new\" not in name) else pd.read_csv(p, low_memory=False)\n",
        "    return strip_and_dedup(df)\n",
        "\n",
        "def read_test_csv(p: str) -> pd.DataFrame:\n",
        "    name = Path(p).name.lower()\n",
        "    try:\n",
        "        df = pd.read_csv(p, skiprows=1, low_memory=False) if \"lable\" in name else pd.read_csv(p, low_memory=False)\n",
        "    except Exception:\n",
        "        df = pd.read_csv(p, low_memory=False)\n",
        "    df = strip_and_dedup(df)\n",
        "    if df.iloc[0].astype(str).str.contains(\"Row|Date|Time|Attack\", case=False, regex=True).any():\n",
        "        df.columns = df.iloc[0].astype(str).tolist()\n",
        "        df = strip_and_dedup(df.iloc[1:].reset_index(drop=True))\n",
        "    return df\n",
        "\n",
        "def extract_label_from_test(df: pd.DataFrame) -> pd.Series:\n",
        "    cands = [c for c in df.columns if re.search(r\"(label|lable|attack)\", c, flags=re.I)]\n",
        "    lab = None\n",
        "    for c in cands:\n",
        "        col = pd.to_numeric(df[c], errors=\"coerce\")\n",
        "        u = set(col.dropna().unique().tolist())\n",
        "        if u.issubset({-1,0,1}) and len(u)>0:\n",
        "            lab = col.map({1:0, -1:1, 0:0}); break\n",
        "    if lab is None: lab = pd.Series(np.zeros(len(df), dtype=np.int64))\n",
        "    return lab.astype(np.int64).rename(\"label\")\n",
        "\n",
        "def numeric_common(train_df: pd.DataFrame, test_df: pd.DataFrame):\n",
        "    drop = {\"Date\",\"Time\",\"datetime\",\"timestamp\",\"Row\",\"row\",\"index\"}\n",
        "    num_tr = [c for c in train_df.columns if pd.api.types.is_numeric_dtype(train_df[c]) and c not in drop]\n",
        "    num_te = [c for c in test_df.columns  if pd.api.types.is_numeric_dtype(test_df[c]) and c not in drop]\n",
        "    feats = sorted(list(set(num_tr) & set(num_te)))\n",
        "    if not feats: raise RuntimeError(\"No shared numeric columns.\")\n",
        "    Xtr = train_df[feats].copy(); Xte = test_df[feats].copy()\n",
        "    for X in (Xtr, Xte):\n",
        "        X.replace([\"?\",\"NA\",\"NaN\",\"nan\",\"\"], np.nan, inplace=True)\n",
        "        for c in X.columns: X[c] = pd.to_numeric(X[c], errors=\"coerce\")\n",
        "        X.interpolate(limit_direction=\"both\", inplace=True)\n",
        "        X.ffill(inplace=True); X.bfill(inplace=True)\n",
        "    return Xtr, Xte, feats\n",
        "\n",
        "def safe_scale(Xtr: pd.DataFrame, Xte: pd.DataFrame, eps: float=1e-8):\n",
        "    std = Xtr.std(0, ddof=0)\n",
        "    keep = std[std > eps].index.tolist()\n",
        "    if len(keep) < len(std): print(f\"[info] dropped {len(std)-len(keep)} low-variance cols\")\n",
        "    mu = Xtr[keep].mean(0); sd = Xtr[keep].std(0, ddof=0).clip(lower=eps)\n",
        "    Xtr = ((Xtr[keep]-mu)/sd).astype(np.float32).replace([np.inf,-np.inf], np.nan).fillna(0.0)\n",
        "    Xte = ((Xte[keep]-mu)/sd).astype(np.float32).replace([np.inf,-np.inf], np.nan).fillna(0.0)\n",
        "    return Xtr, Xte, keep\n",
        "\n",
        "def find_file(cands: List[str], root: str) -> Optional[str]:\n",
        "    low = [c.lower() for c in cands]\n",
        "    for c in cands:\n",
        "        p = Path(root)/c\n",
        "        if p.exists(): return str(p)\n",
        "    for r,_,files in os.walk(root):\n",
        "        for f in files:\n",
        "            if f.lower() in low: return str(Path(r)/f)\n",
        "    for r,_,files in os.walk(root):\n",
        "        for f in files:\n",
        "            fl = f.lower()\n",
        "            for cand in low:\n",
        "                if cand.replace(\"_\",\"\").replace(\"-\",\"\") in fl.replace(\"_\",\"\").replace(\"-\",\"\"):\n",
        "                    return str(Path(r)/f)\n",
        "    return None\n",
        "\n",
        "def download_or_cache() -> str:\n",
        "    try:\n",
        "        import kagglehub\n",
        "    except Exception:\n",
        "        subprocess.check_call([sys.executable,\"-m\",\"pip\",\"install\",\"-q\",\"kagglehub\"]); import kagglehub\n",
        "    print(\"➜ kagglehub ...\")\n",
        "    p = kagglehub.dataset_download(KAGGLE_SLUG)\n",
        "    print(\"[ok]\", p); return p\n",
        "\n",
        "def ensure_preprocessed():\n",
        "    if os.path.exists(TEST_CSV) and os.path.exists(TRAIN_CSV):\n",
        "        print(\"✓ CSVهای از قبل موجودند.\")\n",
        "        return\n",
        "    print(\"➜ دانلود WADI ...\")\n",
        "    root = download_or_cache()\n",
        "    tr = find_file([\"WADI_14days_new.csv\",\"WADI_14days.csv\"], root)\n",
        "    te = find_file([\"WADI_attackdataLABLE.csv\",\"WADI_attackdata.csv\"], root)\n",
        "    if not tr or not te: raise FileNotFoundError(\"WADI CSVs not found\")\n",
        "    train_df = read_train_csv(tr); test_df = read_test_csv(te)\n",
        "    ytest = extract_label_from_test(test_df)\n",
        "    Xtr_raw, Xte_raw, feats = numeric_common(train_df, test_df)\n",
        "    Xtr, Xte, kept = safe_scale(Xtr_raw, Xte_raw)\n",
        "    tr_out = Xtr.copy(); tr_out[\"label\"] = 0\n",
        "    te_out = Xte.copy(); te_out[\"label\"] = ytest.values\n",
        "    tr_out.to_csv(TRAIN_CSV, index=False); te_out.to_csv(TEST_CSV, index=False)\n",
        "    print(\"✓ ساخته شد:\", TRAIN_CSV, TEST_CSV, \"| n_feats:\", len(kept))\n",
        "\n",
        "ensure_preprocessed()\n",
        "\n",
        "# ===================== UNSW-ize (record extraction) =====================\n",
        "df_attack = pd.read_csv(TEST_CSV)\n",
        "FEATURES = [c for c in df_attack.columns if c != \"label\"]\n",
        "X_attack = df_attack[FEATURES].astype(np.float32).values\n",
        "y_attack = df_attack[\"label\"].astype(np.int64).values\n",
        "del df_attack; gc.collect()\n",
        "\n",
        "if DILATE > 0:\n",
        "    k = np.ones(2*DILATE+1, dtype=np.int32)\n",
        "    y_attack = (np.convolve(y_attack, k, mode=\"same\") > 0).astype(np.int64)\n",
        "\n",
        "n_seq_att = len(X_attack) - SEQ_LEN + 1\n",
        "starts_att = np.arange(0, n_seq_att, STRIDE, dtype=np.int64)\n",
        "\n",
        "from numpy.lib.stride_tricks import sliding_window_view\n",
        "y_win_full_att = (sliding_window_view(y_attack, SEQ_LEN).max(axis=1)).astype(np.int64)\n",
        "y_sel_att = y_win_full_att[starts_att]\n",
        "pos_idx_att = starts_att[y_sel_att==1].copy()\n",
        "neg_idx_att = starts_att[y_sel_att==0].copy()\n",
        "\n",
        "diffs = np.abs(np.diff(X_attack, axis=0))\n",
        "act = diffs.mean(axis=1)\n",
        "act_win = sliding_window_view(act, SEQ_LEN-1).mean(axis=1)\n",
        "scores_att = act_win[starts_att]\n",
        "thr_att = np.quantile(scores_att[y_sel_att==0], HARD_NEG_Q)\n",
        "hard_neg_att = neg_idx_att[scores_att[y_sel_att==0] >= thr_att]\n",
        "\n",
        "n_pos = len(pos_idx_att)\n",
        "if n_pos == 0:\n",
        "    raise RuntimeError(\"No positive windows; decrease SEQ_LEN or increase DILATE.\")\n",
        "n_neg = int(max(1, math.ceil(NEG_RATIO * n_pos)))\n",
        "\n",
        "if len(hard_neg_att) >= n_neg:\n",
        "    sel_neg_att = np.random.choice(hard_neg_att, size=n_neg, replace=False)\n",
        "else:\n",
        "    rest_att = np.setdiff1d(neg_idx_att, hard_neg_att)\n",
        "    add_att = np.random.choice(rest_att, size=max(0, n_neg-len(hard_neg_att)), replace=False)\n",
        "    sel_neg_att = np.concatenate([hard_neg_att, add_att]) if len(hard_neg_att)>0 else add_att\n",
        "\n",
        "def make_non_overlapping(idxs: np.ndarray) -> np.ndarray:\n",
        "    if not NON_OVERLAP: return idxs\n",
        "    idxs = np.sort(idxs)\n",
        "    picked, last_end = [], -1\n",
        "    for s in idxs:\n",
        "        if s >= last_end:\n",
        "            picked.append(s); last_end = s + SEQ_LEN\n",
        "    return np.array(picked, dtype=np.int64)\n",
        "\n",
        "pos_idx_att = make_non_overlapping(pos_idx_att)\n",
        "sel_neg_att = make_non_overlapping(sel_neg_att)\n",
        "\n",
        "# negatives from 14-day normal\n",
        "df_trnorm = pd.read_csv(TRAIN_CSV)\n",
        "X_trn = df_trnorm.drop(columns=[\"label\"], errors=\"ignore\").astype(np.float32).values\n",
        "n_seq_tr = len(X_trn) - SEQ_LEN + 1\n",
        "starts_tr = np.arange(0, n_seq_tr, STRIDE, dtype=np.int64)\n",
        "diffs_tr = np.abs(np.diff(X_trn, axis=0))\n",
        "act_tr = diffs_tr.mean(axis=1)\n",
        "act_win_tr = sliding_window_view(act_tr, SEQ_LEN-1).mean(axis=1)\n",
        "scores_tr = act_win_tr[starts_tr]\n",
        "thr_tr = np.quantile(scores_tr, HARD_NEG_Q_TR)\n",
        "hard_neg_tr = starts_tr[scores_tr >= thr_tr]\n",
        "take_tr = int(0.3 * len(sel_neg_att))\n",
        "take_tr = min(take_tr, len(hard_neg_tr))\n",
        "sel_neg_tr = np.random.choice(hard_neg_tr, size=take_tr, replace=False) if take_tr>0 else np.array([], dtype=np.int64)\n",
        "\n",
        "def aggregate_window(block: np.ndarray) -> np.ndarray:\n",
        "    mean = block.mean(0)\n",
        "    std  = block.std(0, ddof=0)\n",
        "    minv = block.min(0)\n",
        "    maxv = block.max(0)\n",
        "    last = block[-1]\n",
        "    delta= last - block[0]\n",
        "    t = np.arange(len(block))\n",
        "    slope = np.polyfit(t, block, deg=1)[0]\n",
        "    q75 = np.quantile(block, 0.75, axis=0); q25 = np.quantile(block, 0.25, axis=0)\n",
        "    iqr = q75 - q25\n",
        "    dsum = np.abs(np.diff(block, axis=0)).sum(0)\n",
        "    return np.concatenate([mean, std, minv, maxv, last, delta, slope, iqr, dsum], axis=0)\n",
        "\n",
        "stat_prefixes = [\"mean\",\"std\",\"min\",\"max\",\"last\",\"delta\",\"slope\",\"iqr\",\"dsum\"]\n",
        "\n",
        "blocks_pos = [aggregate_window(X_attack[s:s+SEQ_LEN,:]) for s in pos_idx_att]\n",
        "y_pos = np.ones(len(blocks_pos), dtype=np.int64)\n",
        "\n",
        "blocks_neg_att = [aggregate_window(X_attack[s:s+SEQ_LEN,:]) for s in sel_neg_att]\n",
        "y_neg_att = np.zeros(len(blocks_neg_att), dtype=np.int64)\n",
        "\n",
        "blocks_neg_tr = [aggregate_window(X_trn[s:s+SEQ_LEN,:]) for s in sel_neg_tr]\n",
        "y_neg_tr = np.zeros(len(blocks_neg_tr), dtype=np.int64)\n",
        "\n",
        "X_agg = np.vstack([np.stack(blocks_pos), np.stack(blocks_neg_att)] + ([np.stack(blocks_neg_tr)] if len(blocks_neg_tr)>0 else []))\n",
        "y_agg = np.concatenate([y_pos, y_neg_att] + ([y_neg_tr] if len(y_neg_tr)>0 else []))\n",
        "\n",
        "cols = []\n",
        "for pfx in stat_prefixes:\n",
        "    cols += [f\"{pfx}_{f}\" for f in FEATURES]\n",
        "df_out = pd.DataFrame(X_agg, columns=cols)\n",
        "df_out[\"label\"] = y_agg\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "tmp_size = VAL_SIZE + TEST_SIZE\n",
        "train_df, tmp_df = train_test_split(df_out, test_size=tmp_size, random_state=SEED, stratify=df_out[\"label\"])\n",
        "rel_test = TEST_SIZE / (VAL_SIZE + TEST_SIZE)\n",
        "val_df, test_df = train_test_split(tmp_df, test_size=rel_test, random_state=SEED, stratify=tmp_df[\"label\"])\n",
        "\n",
        "UNSW_TRAIN = f\"{OUT_DIR_RAW}/wadi_unswized_train.csv\"\n",
        "UNSW_VAL   = f\"{OUT_DIR_RAW}/wadi_unswized_val.csv\"\n",
        "UNSW_TEST  = f\"{OUT_DIR_RAW}/wadi_unswized_test.csv\"\n",
        "train_df.to_csv(UNSW_TRAIN, index=False)\n",
        "val_df.to_csv(UNSW_VAL, index=False)\n",
        "test_df.to_csv(UNSW_TEST, index=False)\n",
        "\n",
        "print(\"UNSW-ized CSVs:\", UNSW_TRAIN, UNSW_VAL, UNSW_TEST)\n",
        "print(\"Shapes:\", train_df.shape, val_df.shape, test_df.shape)\n",
        "print(\"Label ratio train/val/test:\", round(train_df[\"label\"].mean(),3), round(val_df[\"label\"].mean(),3), round(test_df[\"label\"].mean(),3))\n",
        "\n",
        "# ===================== Scaling =====================\n",
        "all_cols = [c for c in train_df.columns if c != \"label\"]\n",
        "Xtr = train_df[all_cols].values.astype(np.float32)\n",
        "Xva = val_df[all_cols].values.astype(np.float32)\n",
        "Xte = test_df[all_cols].values.astype(np.float32)\n",
        "ytr = train_df[\"label\"].values.astype(np.float32)\n",
        "yva = val_df[\"label\"].values.astype(np.float32)\n",
        "yte = test_df[\"label\"].values.astype(np.float32)\n",
        "\n",
        "mu = Xtr.mean(0); sd = Xtr.std(0, ddof=0).clip(min=1e-8)\n",
        "Xtr = ((Xtr - mu)/sd).astype(np.float32)\n",
        "Xva = ((Xva - mu)/sd).astype(np.float32)\n",
        "Xte = ((Xte - mu)/sd).astype(np.float32)\n",
        "Xtr = np.nan_to_num(Xtr, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "Xva = np.nan_to_num(Xva, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "Xte = np.nan_to_num(Xte, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "\n",
        "# ===================== Datasets =====================\n",
        "sensor_names = FEATURES[:]  # original feature (sensor) names\n",
        "PFX = stat_prefixes\n",
        "token_dim = len(PFX)\n",
        "\n",
        "# build fixed index map: for every sensor, indices of its stats in all_cols\n",
        "sensor_to_idx = {}\n",
        "for feat in sensor_names:\n",
        "    idxs = []\n",
        "    ok = True\n",
        "    for p in PFX:\n",
        "        col = f\"{p}_{feat}\"\n",
        "        if col not in all_cols:\n",
        "            ok = False; break\n",
        "        idxs.append(all_cols.index(col))\n",
        "    if ok:\n",
        "        sensor_to_idx[feat] = idxs\n",
        "sensor_order = [s for s in sensor_names if s in sensor_to_idx]\n",
        "S_eff = len(sensor_order)\n",
        "sensor_index_matrix = np.array([sensor_to_idx[s] for s in sensor_order], dtype=np.int64)  # [S_eff, token_dim]\n",
        "\n",
        "def row_to_tokens(row_vec: np.ndarray) -> np.ndarray:\n",
        "    return row_vec[sensor_index_matrix]  # [S_eff, token_dim]\n",
        "\n",
        "class DSMLP(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = torch.from_numpy(X)\n",
        "        self.y = torch.from_numpy(y)\n",
        "    def __len__(self): return len(self.y)\n",
        "    def __getitem__(self,i): return self.X[i], self.y[i]\n",
        "\n",
        "class DSSensors(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        toks = np.stack([row_to_tokens(X[i]) for i in range(len(X))], axis=0)  # [N,S_eff,token_dim]\n",
        "        self.X = torch.from_numpy(toks.astype(np.float32))\n",
        "        self.y = torch.from_numpy(y.astype(np.float32))\n",
        "    def __len__(self): return len(self.y)\n",
        "    def __getitem__(self,i): return self.X[i], self.y[i]\n",
        "\n",
        "if MODEL_TYPE == \"mlp\":\n",
        "    tr_ds = DSMLP(Xtr, ytr); va_ds = DSMLP(Xva, yva); te_ds = DSMLP(Xte, yte)\n",
        "else:\n",
        "    tr_ds = DSSensors(Xtr, ytr); va_ds = DSSensors(Xva, yva); te_ds = DSSensors(Xte, yte)\n",
        "\n",
        "train_loader = DataLoader(tr_ds, batch_size=BATCH_SIZE, shuffle=True,  num_workers=0)\n",
        "val_loader   = DataLoader(va_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
        "test_loader  = DataLoader(te_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
        "\n",
        "# ===================== Models =====================\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, in_dim, p=0.2):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(in_dim, 512), nn.ReLU(), nn.Dropout(p),\n",
        "            nn.Linear(512, 256), nn.ReLU(), nn.Dropout(p),\n",
        "            nn.Linear(256, 1)\n",
        "        )\n",
        "    def forward(self, x): return self.net(x).squeeze(1)\n",
        "\n",
        "class SinusoidalPositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=4096):\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, d_model, dtype=torch.float32)\n",
        "        pos = torch.arange(0, max_len, dtype=torch.float32).unsqueeze(1)\n",
        "        div = torch.exp(torch.arange(0, d_model, 2, dtype=torch.float32) * (-math.log(10000.0)/d_model))\n",
        "        pe[:,0::2] = torch.sin(pos*div); pe[:,1::2] = torch.cos(pos*div)\n",
        "        self.register_buffer(\"pe\", pe.unsqueeze(0))\n",
        "    def forward(self, x): return x + self.pe[:, :x.size(1), :]\n",
        "\n",
        "class TrEncLayer(nn.Module):\n",
        "    def __init__(self, d, nhead, ff, p):\n",
        "        super().__init__()\n",
        "        self.attn = nn.MultiheadAttention(d, nhead, dropout=p, batch_first=True)\n",
        "        self.l1, self.l2 = nn.Linear(d, ff), nn.Linear(ff, d)\n",
        "        self.n1, self.n2 = nn.LayerNorm(d), nn.LayerNorm(d)\n",
        "        self.d1, self.d2 = nn.Dropout(p), nn.Dropout(p)\n",
        "        self.act = nn.GELU()\n",
        "    def forward(self, x):\n",
        "        a,_ = self.attn(x,x,x,need_weights=False)\n",
        "        x = self.n1(x + self.d1(a))\n",
        "        x = self.n2(x + self.d2(self.l2(self.d1(self.act(self.l1(x))))))\n",
        "        return x\n",
        "\n",
        "class TrEnc(nn.Module):\n",
        "    def __init__(self, d, nhead, nlayer, ff, p):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([TrEncLayer(d, nhead, ff, p) for _ in range(nlayer)])\n",
        "    def forward(self, x):\n",
        "        for lyr in self.layers: x = lyr(x)\n",
        "        return x\n",
        "\n",
        "class TrBiLSTM_Sensors_Att(nn.Module):\n",
        "    def __init__(self, token_dim, d_model, nhead, nlay_tr, lstm_h, nlay_lstm, p=0.1):\n",
        "        super().__init__()\n",
        "        self.proj = nn.Linear(token_dim, d_model)\n",
        "        self.pos  = SinusoidalPositionalEncoding(d_model)\n",
        "        self.enc  = TrEnc(d_model, nhead, nlay_tr, d_model*2, p)\n",
        "        self.lstm = nn.LSTM(d_model, lstm_h, num_layers=nlay_lstm, batch_first=True,\n",
        "                            bidirectional=True, dropout=p if nlay_lstm>1 else 0.0)\n",
        "        self.drop = nn.Dropout(p)\n",
        "        self.att  = nn.Sequential(\n",
        "            nn.Linear(lstm_h*2, d_model//2),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(d_model//2, 1)\n",
        "        )\n",
        "        self.fc   = nn.Linear(lstm_h*2, 1)\n",
        "    def forward(self, x):              # x: [B,S,token_dim]\n",
        "        x = self.proj(x)               # [B,S,D]\n",
        "        x = self.pos(x)\n",
        "        x = self.enc(x)\n",
        "        x,_ = self.lstm(x)             # [B,S,2H]\n",
        "        x = self.drop(x)\n",
        "        w = torch.softmax(self.att(x).squeeze(-1), dim=1)  # [B,S]\n",
        "        x = (x * w.unsqueeze(-1)).sum(1)                   # [B,2H]\n",
        "        return self.fc(x).squeeze(1)\n",
        "\n",
        "if MODEL_TYPE == \"mlp\":\n",
        "    model = MLP(Xtr.shape[1], p=DROPOUT).to(DEVICE)\n",
        "else:\n",
        "    model = TrBiLSTM_Sensors_Att(token_dim=token_dim, d_model=TR_DIM, nhead=NHEAD, nlay_tr=N_LAY_TR,\n",
        "                                 lstm_h=LSTM_H, nlay_lstm=N_LAY_LSTM, p=DROPOUT).to(DEVICE)\n",
        "\n",
        "print(model)\n",
        "print(\"Total params:\", sum(p.numel() for p in model.parameters()))\n",
        "\n",
        "# ===================== Loss/Opt =====================\n",
        "class FocalBCEWithLogitsLoss(nn.Module):\n",
        "    def __init__(self, alpha=0.6, gamma=2.0):\n",
        "        super().__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.bce = nn.BCEWithLogitsLoss(reduction=\"none\")\n",
        "    def forward(self, logits, targets):\n",
        "        loss_bce = self.bce(logits, targets)\n",
        "        p = torch.sigmoid(logits)\n",
        "        pt = torch.where(targets==1.0, p, 1.0-p)\n",
        "        mod = (1.0 - pt).pow(self.gamma)\n",
        "        loss = self.alpha * mod * loss_bce\n",
        "        return loss.mean()\n",
        "\n",
        "crit = FocalBCEWithLogitsLoss(alpha=0.6, gamma=2.0)\n",
        "opt  = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
        "\n",
        "def run_epoch(loader, train=False):\n",
        "    (model.train if train else model.eval)()\n",
        "    total, Ys, Ps = 0.0, [], []\n",
        "    for xb, yb in loader:\n",
        "        xb = xb.to(DEVICE); yb = yb.to(DEVICE)\n",
        "        with torch.set_grad_enabled(train):\n",
        "            logit = model(xb)\n",
        "            loss = crit(logit, yb)\n",
        "            if train:\n",
        "                opt.zero_grad(set_to_none=True); loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "                opt.step()\n",
        "        total += loss.item() * len(yb)\n",
        "        Ps.append(torch.sigmoid(logit).detach().cpu().numpy())\n",
        "        Ys.append(yb.detach().cpu().numpy())\n",
        "    y = np.concatenate(Ys) if Ys else np.array([])\n",
        "    p = np.concatenate(Ps) if Ps else np.array([])\n",
        "    pred = (p >= 0.5).astype(int) if len(p) else np.array([])\n",
        "    acc = accuracy_score(y, pred) if len(y) else 0.0\n",
        "    f1  = f1_score(y, pred, zero_division=0) if len(y) else 0.0\n",
        "    return total/max(1,len(loader.dataset)), acc, f1, y, p\n",
        "\n",
        "def pick_threshold_constrained(y, p, min_prec=0.6):\n",
        "    prec, rec, thr = precision_recall_curve(y, p)\n",
        "    best_t, best_f1 = None, -1.0\n",
        "    for pr, rc, t in zip(prec[:-1], rec[:-1], thr):\n",
        "        if pr >= min_prec:\n",
        "            f1 = 0.0 if (pr+rc)==0 else 2*pr*rc/(pr+rc)\n",
        "            if f1 > best_f1:\n",
        "                best_f1, best_t = f1, float(t)\n",
        "    if best_t is None:\n",
        "        qs = np.quantile(p, np.linspace(0.4,0.9,6))\n",
        "        f1s = [f1_score(y, (p>=t).astype(int), zero_division=0) for t in qs]\n",
        "        best_t = float(qs[int(np.argmax(f1s))])\n",
        "    return best_t, float(f1_score(y, (p>=best_t).astype(int), zero_division=0))\n",
        "\n",
        "best_f1, best_thr, patience, noimp = -1.0, 0.5, EARLY_PATIENCE, 0\n",
        "for e in range(1, EPOCHS+1):\n",
        "    t0 = time.time()\n",
        "    tr_loss, tr_acc, tr_f1, _, _ = run_epoch(train_loader, True)\n",
        "    va_loss, va_acc, va_f1, yv, pv = run_epoch(val_loader, False)\n",
        "    thr_opt, f1_opt = pick_threshold_constrained(yv, pv, min_prec=MIN_PREC_AT_TUNE)\n",
        "    print(f\"Epoch {e:02d}/{EPOCHS} | {time.time()-t0:.1f}s | train {tr_loss:.4f}/{tr_acc:.3f}/{tr_f1:.3f} | val f1@0.5 {va_f1:.3f} | best f1 {f1_opt:.3f} @ thr={thr_opt:.2f}\")\n",
        "    if f1_opt > best_f1:\n",
        "        best_f1, best_thr, noimp = f1_opt, thr_opt, 0\n",
        "        torch.save({\"model\": model.state_dict(), \"thr\": best_thr}, CKPT)\n",
        "        print(\"  ✓ Saved:\", CKPT)\n",
        "    else:\n",
        "        noimp += 1\n",
        "        if noimp >= patience:\n",
        "            print(\"Early stopping.\"); break\n",
        "\n",
        "ckpt = torch.load(CKPT, map_location=DEVICE)\n",
        "model.load_state_dict(ckpt[\"model\"]); best_thr = ckpt[\"thr\"]\n",
        "\n",
        "def evaluate(loader, thr):\n",
        "    model.eval(); Ys, Ps = [], []\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in loader:\n",
        "            xb = xb.to(DEVICE)\n",
        "            pr = torch.sigmoid(model(xb)).cpu().numpy()\n",
        "            Ys.append(yb.numpy()); Ps.append(pr)\n",
        "    y = np.concatenate(Ys); p = np.concatenate(Ps)\n",
        "    pred = (p >= thr).astype(int)\n",
        "    acc = accuracy_score(y, pred); f1 = f1_score(y, pred, zero_division=0)\n",
        "    return acc, f1, y, pred\n",
        "\n",
        "acc, f1, y_true, y_pred = evaluate(test_loader, best_thr)\n",
        "print(\"\\n--- Test (UNSW-ized records) ---\")\n",
        "print({\"acc\": round(acc,4), \"f1\": round(f1,4), \"thr\": round(best_thr,3)})\n",
        "print(confusion_matrix(y_true, y_pred))\n",
        "print(classification_report(y_true, y_pred, target_names=[\"Normal (0)\",\"Attack (1)\"], zero_division=0))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "57YN4jYj5OmT",
        "outputId": "57e21acd-74d5-442a-b802-e5f551857d9a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n",
            "➜ دانلود WADI ...\n",
            "➜ kagglehub ...\n",
            "Using Colab cache for faster access to the 'wadi-data' dataset.\n",
            "[ok] /kaggle/input/wadi-data\n",
            "[info] dropped 34 low-variance cols\n",
            "✓ ساخته شد: ./wadi_preprocessed/wadi_train_preprocessed.csv ./wadi_preprocessed/wadi_test_preprocessed.csv | n_feats: 93\n",
            "UNSW-ized CSVs: ./wadi_preprocessed/wadi_unswized_train.csv ./wadi_preprocessed/wadi_unswized_val.csv ./wadi_preprocessed/wadi_unswized_test.csv\n",
            "Shapes: (27011, 838) (5788, 838) (5789, 838)\n",
            "Label ratio train/val/test: 0.278 0.278 0.278\n",
            "TrBiLSTM_Sensors_Att(\n",
            "  (proj): Linear(in_features=9, out_features=192, bias=True)\n",
            "  (pos): SinusoidalPositionalEncoding()\n",
            "  (enc): TrEnc(\n",
            "    (layers): ModuleList(\n",
            "      (0-1): 2 x TrEncLayer(\n",
            "        (attn): MultiheadAttention(\n",
            "          (out_proj): NonDynamicallyQuantizableLinear(in_features=192, out_features=192, bias=True)\n",
            "        )\n",
            "        (l1): Linear(in_features=192, out_features=384, bias=True)\n",
            "        (l2): Linear(in_features=384, out_features=192, bias=True)\n",
            "        (n1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
            "        (n2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
            "        (d1): Dropout(p=0.15, inplace=False)\n",
            "        (d2): Dropout(p=0.15, inplace=False)\n",
            "        (act): GELU(approximate='none')\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (lstm): LSTM(192, 192, batch_first=True, bidirectional=True)\n",
            "  (drop): Dropout(p=0.15, inplace=False)\n",
            "  (att): Sequential(\n",
            "    (0): Linear(in_features=384, out_features=96, bias=True)\n",
            "    (1): GELU(approximate='none')\n",
            "    (2): Linear(in_features=96, out_features=1, bias=True)\n",
            "  )\n",
            "  (fc): Linear(in_features=384, out_features=1, bias=True)\n",
            ")\n",
            "Total params: 1226306\n",
            "Epoch 01/20 | 23.6s | train 0.0765/0.754/0.382 | val f1@0.5 0.602 | best f1 0.700 @ thr=0.38\n",
            "  ✓ Saved: ./bilstm_trf_checkpoints/best_unswized_trf_bilstm_sensors.pt\n",
            "Epoch 02/20 | 22.7s | train 0.0456/0.872/0.752 | val f1@0.5 0.865 | best f1 0.882 @ thr=0.39\n",
            "  ✓ Saved: ./bilstm_trf_checkpoints/best_unswized_trf_bilstm_sensors.pt\n",
            "Epoch 03/20 | 22.9s | train 0.0218/0.944/0.898 | val f1@0.5 0.922 | best f1 0.953 @ thr=0.63\n",
            "  ✓ Saved: ./bilstm_trf_checkpoints/best_unswized_trf_bilstm_sensors.pt\n",
            "Epoch 04/20 | 23.3s | train 0.0128/0.970/0.946 | val f1@0.5 0.970 | best f1 0.974 @ thr=0.43\n",
            "  ✓ Saved: ./bilstm_trf_checkpoints/best_unswized_trf_bilstm_sensors.pt\n",
            "Epoch 05/20 | 23.9s | train 0.0097/0.977/0.959 | val f1@0.5 0.980 | best f1 0.981 @ thr=0.55\n",
            "  ✓ Saved: ./bilstm_trf_checkpoints/best_unswized_trf_bilstm_sensors.pt\n",
            "Epoch 06/20 | 23.5s | train 0.0056/0.988/0.978 | val f1@0.5 0.984 | best f1 0.986 @ thr=0.66\n",
            "  ✓ Saved: ./bilstm_trf_checkpoints/best_unswized_trf_bilstm_sensors.pt\n",
            "Epoch 07/20 | 23.6s | train 0.0052/0.987/0.977 | val f1@0.5 0.989 | best f1 0.990 @ thr=0.54\n",
            "  ✓ Saved: ./bilstm_trf_checkpoints/best_unswized_trf_bilstm_sensors.pt\n",
            "Epoch 08/20 | 23.5s | train 0.0031/0.993/0.987 | val f1@0.5 0.990 | best f1 0.991 @ thr=0.38\n",
            "  ✓ Saved: ./bilstm_trf_checkpoints/best_unswized_trf_bilstm_sensors.pt\n",
            "Epoch 09/20 | 23.6s | train 0.0034/0.992/0.986 | val f1@0.5 0.984 | best f1 0.991 @ thr=0.64\n",
            "Epoch 10/20 | 23.5s | train 0.0027/0.994/0.989 | val f1@0.5 0.993 | best f1 0.994 @ thr=0.43\n",
            "  ✓ Saved: ./bilstm_trf_checkpoints/best_unswized_trf_bilstm_sensors.pt\n",
            "Epoch 11/20 | 23.5s | train 0.0025/0.994/0.990 | val f1@0.5 0.988 | best f1 0.991 @ thr=0.28\n",
            "Epoch 12/20 | 23.5s | train 0.0025/0.994/0.989 | val f1@0.5 0.993 | best f1 0.996 @ thr=0.61\n",
            "  ✓ Saved: ./bilstm_trf_checkpoints/best_unswized_trf_bilstm_sensors.pt\n",
            "Epoch 13/20 | 23.7s | train 0.0020/0.996/0.993 | val f1@0.5 0.988 | best f1 0.993 @ thr=0.28\n",
            "Epoch 14/20 | 23.5s | train 0.0025/0.994/0.990 | val f1@0.5 0.987 | best f1 0.988 @ thr=0.41\n",
            "Epoch 15/20 | 23.5s | train 0.0024/0.995/0.991 | val f1@0.5 0.991 | best f1 0.995 @ thr=0.60\n",
            "Epoch 16/20 | 23.5s | train 0.0022/0.995/0.991 | val f1@0.5 0.993 | best f1 0.993 @ thr=0.51\n",
            "Early stopping.\n",
            "\n",
            "--- Test (UNSW-ized records) ---\n",
            "{'acc': 0.9978, 'f1': 0.996, 'thr': 0.611}\n",
            "[[4169   12]\n",
            " [   1 1607]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "  Normal (0)       1.00      1.00      1.00      4181\n",
            "  Attack (1)       0.99      1.00      1.00      1608\n",
            "\n",
            "    accuracy                           1.00      5789\n",
            "   macro avg       1.00      1.00      1.00      5789\n",
            "weighted avg       1.00      1.00      1.00      5789\n",
            "\n"
          ]
        }
      ]
    }
  ]
}